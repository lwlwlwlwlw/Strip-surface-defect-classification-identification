{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 带钢表面缺陷分类识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.导入相关的Python包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split \u001b[38;5;66;03m#训练集，测试集划分函数\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RFECV\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split #训练集，测试集划分函数\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### 2.数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Faults27x7_var'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 以 DataFrame 的形式导入标签文件\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m labels_pd \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFaults27x7_var\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 提取出标签文件的数据\u001b[39;00m\n\u001b[0;32m      6\u001b[0m labels_data \u001b[38;5;241m=\u001b[39m labels_pd\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\env_da\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\env_da\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    664\u001b[0m     dialect,\n\u001b[0;32m    665\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    675\u001b[0m )\n\u001b[0;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\env_da\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\env_da\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\env_da\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\env_da\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Faults27x7_var'"
     ]
    }
   ],
   "source": [
    "# 屏蔽所有警告信息\n",
    "warnings.filterwarnings('ignore')\n",
    "# 以 DataFrame 的形式导入标签文件\n",
    "labels_pd = pd.read_csv('Faults27x7_var', header=None, prefix=\"0\")\n",
    "# 提取出标签文件的数据\n",
    "labels_data = labels_pd.values\n",
    "# 以 DataFrame的形式导入数据集\n",
    "dataset = pd.read_table('Faults.tsv', header=None, prefix=\"0\")\n",
    "# 设置数据集的标签\n",
    "feature_names = ['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas']\n",
    "class_names = ['Pastry','Z_Scratch','K_Scratch','Stains','Dirtiness','Bumps','Other_Faults']\n",
    "columns = ['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas', 'Pastry', 'Z_Scratch', 'K_Scratch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "dataset.columns = columns\n",
    "# 查看数据集\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 查看数据基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 查看数据集大小\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 查看数据集是否有缺失值和离散值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 查看数据集是否有缺失值\n",
    "dataset.shape[0] - dataset.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 查看数据集是否有离散值\n",
    "# 1.箱型图法\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(1, 28, 1):\n",
    "    plt.subplot(3, 10, i)\n",
    "    plt.boxplot(dataset[dataset.columns[i - 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2.局部离群值因子法(LOF)\n",
    "clf = LocalOutlierFactor(n_neighbors=20)  # 近邻数是超参数，20为默认值，一般取该值效果不错\n",
    "dataset_LOF_copy = dataset[['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas']].copy()\n",
    "y_pred = clf.fit_predict(np.array(dataset_LOF_copy))  # 结果中-1表离群样本，1表正常样本\n",
    "LOF_arr = dataset[['Pastry', 'Z_Scratch', 'K_Scratch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']][y_pred==-1]\n",
    "LOF_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset\u001b[38;5;241m=\u001b[39m\u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(index\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1268\u001b[39m,\u001b[38;5;241m1941\u001b[39m)])\n\u001b[0;32m      2\u001b[0m dataset\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset=dataset.drop(index=dataset.index[range(1268,1941)])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 将输入特征与输出特征分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "6           673\n",
       "5           402\n",
       "2           391\n",
       "1           190\n",
       "0           158\n",
       "3            72\n",
       "4            55\n",
       "dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset[['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas']]\n",
    "# Y = dataset[['Pastry','Z_Scratch','K_Scratch','Stains','Dirtiness','Bumps','Other_Faults']]\n",
    "# 将dataset数据集转为二维数组\n",
    "dataset_data = dataset.values\n",
    "# 将原数据集的Y化简到1维，Pastry为1，Z_Scratch为2，以此类推直到7\n",
    "Y_data = []\n",
    "for i in range(len(dataset_data)):\n",
    "    for j in range(27, 34, 1):\n",
    "        if (dataset_data[i][j] == 1):\n",
    "           Y_data.append(j - 27)\n",
    "# # # 将Y的数据转为英文，便于分析\n",
    "# # for i in range(len(Y_data)):\n",
    "# #     if (Y_data[i] == 1):\n",
    "# #         Y_data[i] = 'Pastry'\n",
    "# #     elif (Y_data[i] == 2):\n",
    "# #         Y_data[i] = 'Z_Scratch'\n",
    "# #     elif (Y_data[i] == 3):\n",
    "# #         Y_data[i] = 'K_Scratch'\n",
    "# #     elif (Y_data[i] == 4):\n",
    "# #         Y_data[i] = 'Stains'\n",
    "# #     elif (Y_data[i] == 5):\n",
    "# #         Y_data[i] = 'Dirtiness'\n",
    "# #     elif (Y_data[i] == 6):\n",
    "# #         Y_data[i] = 'Bumps'\n",
    "# #     elif (Y_data[i] == 7):\n",
    "# #         Y_data[i] = 'Other_Faults'\n",
    "# 输出集\n",
    "Y = pd.DataFrame(Y_data)\n",
    "Y.columns = ['category']\n",
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 分离测试集和训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "6           605\n",
       "5           362\n",
       "2           352\n",
       "1           171\n",
       "0           142\n",
       "3            65\n",
       "4            49\n",
       "dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=3, stratify=Y)\n",
    "# test_size:  若为小数，则为测试集样本占总样本的比例；若为整数，则为测试集样本数量\n",
    "# random_state：随机数的种子，在需要重复试验的时候，指定一个固定值，则得到相同的划分结果\n",
    "# stratify：划分前是否打乱原始样本顺序\n",
    "y_train.value_counts()  # 训练集中各类样本数量统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 对输入数据做标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 利用训练集的参数来标准化测试集\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_train.columns = feature_names\n",
    "X_test.columns = feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "X_train = SelectKBest(f_classif, k=15).fit_transform(X_train, y_train)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_train.columns = [['X_Minimum', 'X_Maximum', 'Pixels_Areas', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Length_of_Conveyer','TypeOfSteel_A300', 'TypeOfSteel_A400', 'Outside_X_Index','Edges_Y_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'SigmoidOfAreas']]\n",
    "X_test = X_test[['X_Minimum', 'X_Maximum', 'Pixels_Areas', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Length_of_Conveyer','TypeOfSteel_A300', 'TypeOfSteel_A400', 'Outside_X_Index','Edges_Y_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'SigmoidOfAreas']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 构造模型评价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def showCVResults(cv_results):\n",
    "    print('fit_time:%.4f' % cv_results['fit_time'].mean())\n",
    "    print('score_time:%.4f' % cv_results['score_time'].mean())\n",
    "    print('accuracy:%.4f' % cv_results['test_accuracy'].mean())\n",
    "    print('precision_weighted:%.4f' % cv_results['test_precision_weighted'].mean())\n",
    "    print('recall_weighted:%.4f' % cv_results['test_recall_weighted'].mean())\n",
    "    print('f1_weighted:%.4f' % cv_results['test_f1_weighted'].mean())\n",
    "    print('precision_macro:%.4f' % cv_results['test_precision_macro'].mean())\n",
    "    print('recall_macro:%.4f' % cv_results['test_recall_macro'].mean())\n",
    "    print('f1_macro:%.4f' % cv_results['test_f1_macro'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 构造分层的交叉验证迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=2)\n",
    "# 分层划分训练集和测试集，使训练集、测试集中各类别样本数量比例与原始数据集保持一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "6           68\n",
       "5           40\n",
       "2           39\n",
       "1           19\n",
       "0           16\n",
       "3            7\n",
       "4            6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集中各类样本数量统计\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "0           68\n",
       "1           68\n",
       "2           68\n",
       "3           68\n",
       "4           68\n",
       "5           68\n",
       "6           68\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = SMOTE(random_state=42, n_jobs=-1)\n",
    "X_test, y_test = sm.fit_resample(X_test, y_test)\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.使用决策树进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time:0.0436\n",
      "score_time:0.0122\n",
      "accuracy:0.7218\n",
      "precision_weighted:0.7246\n",
      "recall_weighted:0.7218\n",
      "f1_weighted:0.7200\n",
      "precision_macro:0.7461\n",
      "recall_macro:0.7519\n",
      "f1_macro:0.7438\n"
     ]
    }
   ],
   "source": [
    "# 不做任何剪枝操作的决策树拟合\n",
    "dtc = DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\")\n",
    "# 执行交叉验证并进行多指标评估\n",
    "cv_results_dt = cross_validate(estimator=dtc, X=X, y=Y, cv=skf, scoring=[\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\", \"precision_macro\", \"recall_macro\", \"f1_macro\"])\n",
    "showCVResults(cv_results_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.56      0.59        18\n",
      "           1       0.79      0.79      0.79        19\n",
      "           2       0.97      0.88      0.93        43\n",
      "           3       1.00      0.88      0.93         8\n",
      "           4       0.50      0.50      0.50         6\n",
      "           5       0.53      0.64      0.58        33\n",
      "           6       0.69      0.69      0.69        68\n",
      "\n",
      "    accuracy                           0.72       195\n",
      "   macro avg       0.73      0.70      0.71       195\n",
      "weighted avg       0.74      0.72      0.73       195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\")\n",
    "dtc.fit(X_train, y_train)\n",
    "y_pred = dtc.predict(X_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 将决策树进行完整的可视化\n",
    "dtc.fit(X_train, y_train)\n",
    "export_graphviz(dtc, out_file=\"D://tree.dot\",\n",
    "                feature_names=feature_names, class_names=class_names,\n",
    "                filled=True, rounded=True,special_characters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.00012999 0.00013637 0.00014026 0.00014773 0.00014876\n",
      " 0.00015    0.00015341 0.00015455 0.00015546 0.00015652 0.00015709\n",
      " 0.00015853 0.00015965 0.0001612  0.00016237 0.00016364 0.00016364\n",
      " 0.00016364 0.00016364 0.00016364 0.00016364 0.00016364 0.00016364\n",
      " 0.00016364 0.00016364 0.00016364 0.00016364 0.00016364 0.00016364\n",
      " 0.00016364 0.00016364 0.00018909 0.00019088 0.00020083 0.0002026\n",
      " 0.00020847 0.00021273 0.00021819 0.00021819 0.00021819 0.00021819\n",
      " 0.00021819 0.00021819 0.00021819 0.00021819 0.00021819 0.00022468\n",
      " 0.00022562 0.00023564 0.00023662 0.00024546 0.00024546 0.00024546\n",
      " 0.00024546 0.00024546 0.00024546 0.00024546 0.00024546 0.00025091\n",
      " 0.0002513  0.00026182 0.00026182 0.00026182 0.00026182 0.00026182\n",
      " 0.00026182 0.00026494 0.00026564 0.00027    0.00027137 0.00027273\n",
      " 0.00027273 0.00027273 0.00027273 0.00027273 0.00027429 0.00027621\n",
      " 0.00027775 0.00027903 0.00028052 0.00028052 0.00028052 0.00028364\n",
      " 0.00029269 0.00029455 0.00029455 0.00029644 0.00029784 0.0002984\n",
      " 0.00029923 0.00030004 0.0003021  0.00030409 0.00030517 0.00030682\n",
      " 0.00031091 0.00031419 0.00031993 0.00032153 0.00032182 0.00032687\n",
      " 0.00032728 0.00032728 0.00032728 0.00032728 0.00032728 0.0003274\n",
      " 0.00035083 0.00035455 0.00036234 0.00036465 0.00038366 0.00039273\n",
      " 0.00039273 0.0004091  0.00041328 0.00041512 0.00041592 0.00042026\n",
      " 0.00042079 0.00042079 0.00042546 0.00043204 0.00043637 0.00043637\n",
      " 0.00043637 0.00044644 0.00045196 0.0004572  0.00045819 0.00046769\n",
      " 0.00047225 0.00048069 0.00049302 0.00049603 0.00050001 0.00050366\n",
      " 0.0005091  0.00051615 0.00052131 0.00052209 0.00055055 0.00055309\n",
      " 0.00055338 0.00056133 0.00056261 0.00056884 0.00057918 0.00058202\n",
      " 0.00061957 0.00062088 0.00064974 0.00065516 0.00066803 0.00067472\n",
      " 0.00068401 0.00068482 0.00071376 0.00072547 0.0007462  0.00076683\n",
      " 0.00076945 0.00078532 0.00081029 0.00090588 0.00091063 0.00094468\n",
      " 0.00100369 0.00101586 0.00107897 0.00111694 0.00115891 0.00124408\n",
      " 0.00126925 0.00130331 0.00140415 0.0014584  0.00160543 0.00165494\n",
      " 0.00167934 0.00170784 0.00172036 0.00177209 0.0021121  0.00269373\n",
      " 0.00296218 0.00297926 0.00349435 0.00359079 0.00443906 0.00505013\n",
      " 0.00514209 0.00639781 0.00944773 0.01050523 0.03949726]\n",
      "Number of nodes in the last tree is: 1 with ccp_alpha: 0.03949726395367009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23020ac1490>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1280x960 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 对决策树进行预剪枝和后剪枝\n",
    "# 后剪枝\n",
    "path = dtc.cost_complexity_pruning_path(X_train, y_train)\n",
    "#cost_complexity_pruning_path：返回两个参数，\n",
    "#第一个是CCP剪枝后决策树序列T0,T1,...,Tt对应的误差率增益率α；第二个是剪枝后决策树所有叶子节点的不纯度\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "print(ccp_alphas)\n",
    "# ccp_alpha_list = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "#输出最后一个数的节点个数和ccp_alphas\n",
    "print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "      clfs[-1].tree_.node_count, ccp_alphas[-1]))\n",
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "plt.rcParams['savefig.dpi'] = 80 #图片像素\n",
    "plt.rcParams['figure.dpi'] = 200 #分辨率\n",
    "# 默认的像素：[6.0,4.0]，分辨率为100，图片尺寸为 600*400\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23020b1feb0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1280x960 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 预剪枝\n",
    "max_depth = np.arange(1, 20, 1)\n",
    "dtcs = []\n",
    "for i in max_depth:\n",
    "    dtc = DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", max_depth=i)\n",
    "    dtc.fit(X_train, y_train)\n",
    "    dtcs.append(dtc)\n",
    "train_scores = [dtc.score(X_train, y_train) for dtc in dtcs]\n",
    "test_scores = [dtc.score(X_test, y_test) for dtc in dtcs]\n",
    "plt.rcParams['savefig.dpi'] = 80 #图片像素\n",
    "plt.rcParams['figure.dpi'] = 200 #分辨率\n",
    "# 默认的像素：[6.0,4.0]，分辨率为100，图片尺寸为 600*400\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"max_depth\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs max_depth for training and testing sets\")\n",
    "ax.plot(max_depth, train_scores, marker='', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(max_depth, test_scores, marker='', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7377821834522865\n",
      "{'ccp_alpha': 0, 'max_depth': 9}\n"
     ]
    }
   ],
   "source": [
    "# 使用网格搜索法寻求最优参数\n",
    "dtc = DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\")\n",
    "param_grid = [\n",
    "    {\n",
    "        'max_depth': np.arange(1, 20, 1),\n",
    "        'ccp_alpha': [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "    }\n",
    "]\n",
    "grid_search = GridSearchCV(estimator=dtc, param_grid=param_grid, scoring=\"accuracy\", cv=skf)\n",
    "grid_search.fit(X, Y)\n",
    "# 最佳模型得分\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# 最佳的模型参数\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.53      0.52        15\n",
      "           1       0.84      0.89      0.86        18\n",
      "           2       0.90      0.97      0.93        36\n",
      "           3       1.00      0.88      0.93         8\n",
      "           4       0.33      0.67      0.44         3\n",
      "           5       0.62      0.64      0.63        39\n",
      "           6       0.76      0.68      0.72        76\n",
      "\n",
      "    accuracy                           0.74       195\n",
      "   macro avg       0.71      0.75      0.72       195\n",
      "weighted avg       0.75      0.74      0.75       195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", max_depth=9, ccp_alpha=0)\n",
    "dtc.fit(X_train, y_train)\n",
    "y_pred = dtc.predict(X_test)\n",
    "print(classification_report(y_pred, y_test))\n",
    "# 执行交叉验证并进行多指标评估\n",
    "# cv_results_dt = cross_validate(estimator=dtc, X=X, y=Y, cv=skf, scoring=[\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\", \"precision_macro\", \"recall_macro\", \"f1_macro\"])\n",
    "# showCVResults(cv_results_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 将决策树进行完整的可视化\n",
    "dtc.fit(X_train, y_train)\n",
    "export_graphviz(dtc, out_file=\"D://tree.dot\",\n",
    "                feature_names=feature_names,class_names=class_names,\n",
    "                filled=True, rounded=True,special_characters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.支持向量机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (1746, 7) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 0.1 5 0.1 0.1 5 0.1 gamma=0.1 C=4.6 0.7743\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model_svm \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.6\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel_svm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_svm\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_pred, y_test))\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\sklearn\\svm\\_base.py:173\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    171\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    184\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m    185\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    186\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\sklearn\\base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    594\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\sklearn\\utils\\validation.py:1090\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1071\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1072\u001b[0m     )\n\u001b[0;32m   1074\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1075\u001b[0m     X,\n\u001b[0;32m   1076\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1087\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1088\u001b[0m )\n\u001b[1;32m-> 1090\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1092\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\sklearn\\utils\\validation.py:1111\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1110\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m-> 1111\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1112\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[0;32m   1113\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\sklearn\\utils\\validation.py:1156\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m   1147\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1148\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1153\u001b[0m         )\n\u001b[0;32m   1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[1;32m-> 1156\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[0;32m   1158\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (1746, 7) instead."
     ]
    }
   ],
   "source": [
    "# 0.1 5 0.1 0.1 5 0.1 gamma=0.1 C=4.6 0.7743\n",
    "model_svm = SVC(kernel='rbf', gamma=0.1, C=4.6)\n",
    "model_svm.fit(X_train, y_train)\n",
    "y_pred = model_svm.predict(X_test)\n",
    "print(classification_report(y_pred, y_test))\n",
    "# cv_results_lr = cross_validate(estimator=model_svm, X=X, y=Y, cv=skf, scoring=[\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\", \"precision_macro\", \"recall_macro\", \"f1_macro\"])\n",
    "# showCVResults(cv_results_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.60      0.58        15\n",
      "           1       0.79      0.79      0.79        19\n",
      "           2       1.00      0.97      0.99        40\n",
      "           3       1.00      0.78      0.88         9\n",
      "           4       0.67      0.57      0.62         7\n",
      "           5       0.68      0.73      0.70        37\n",
      "           6       0.72      0.72      0.72        68\n",
      "\n",
      "    accuracy                           0.77       195\n",
      "   macro avg       0.77      0.74      0.75       195\n",
      "weighted avg       0.77      0.77      0.77       195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear_svm = SVC(kernel='linear', C=6.7)\n",
    "linear_svm.fit(X_train, y_train)\n",
    "y_pred = model_svm.predict(X_test)\n",
    "print(classification_report(y_pred, y_test))\n",
    "# cv_results_lr = cross_validate(estimator=linear_svm, X=X, y=Y, cv=skf, scoring=[\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\", \"precision_macro\", \"recall_macro\", \"f1_macro\"])\n",
    "# showCVResults(cv_results_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7611674171101106\n",
      "{'gamma': 0.1, 'C': 4.6}\n"
     ]
    }
   ],
   "source": [
    "C_arr = np.arange(0.1, 5, 0.1)\n",
    "gamma_arr = np.arange(0.1, 5, 0.1)\n",
    "param_grid = [\n",
    "    {\n",
    "        'C': C_arr,\n",
    "        'gamma': gamma_arr\n",
    "    }\n",
    "]\n",
    "random_search = RandomizedSearchCV(estimator=model_svm, param_distributions=param_grid, n_iter=100, cv=5, random_state=42)\n",
    "# grid_search = GridSearchCV(estimator=model_svm, param_grid=param_grid, scoring=\"accuracy\", cv=skf)\n",
    "random_search.fit(X_train, y_train)\n",
    "# 最佳模型得分\n",
    "print(random_search.best_score_)\n",
    "# 最佳的模型参数\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6141361672806297\n",
      "{'C': 6.7}\n",
      "0.6141361672806297\n",
      "{'C': 6.7}\n"
     ]
    }
   ],
   "source": [
    "C_arr = np.arange(0.1, 10, 0.1)\n",
    "param_grid = [\n",
    "    {\n",
    "        'C': C_arr,\n",
    "    }\n",
    "]\n",
    "random_search = RandomizedSearchCV(estimator=linear_svm, param_distributions=param_grid, n_iter=100, cv=5, random_state=42)\n",
    "# grid_search = GridSearchCV(estimator=model_svm, param_grid=param_grid, scoring=\"accuracy\", cv=skf)\n",
    "random_search.fit(X, Y)\n",
    "# 最佳模型得分\n",
    "print(random_search.best_score_)\n",
    "# 最佳的模型参数\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import random\n",
    "\n",
    "\n",
    "# 异常类\n",
    "class ReliefError:\n",
    "    pass\n",
    "\n",
    "\n",
    "class Relief:\n",
    "    def __init__(self, data_df, sample_rate, t, k):\n",
    "        \"\"\"\n",
    "        #\n",
    "        :param data_df: 数据框（字段为特征，行为样本）\n",
    "        :param sample_rate: 抽样比例\n",
    "        :param t: 统计量分量阈值\n",
    "        :param k: k近邻的个数\n",
    "        \"\"\"\n",
    "        self.__data = data_df\n",
    "        self.__feature = data_df.columns\n",
    "        self.__sample_num = int(round(len(data_df) * sample_rate))\n",
    "        self.__t = t\n",
    "        self.__k = k\n",
    "\n",
    "    # 数据处理（将离散型数据处理成连续型数据，比如字符到数值）\n",
    "    def get_data(self):\n",
    "        new_data = pd.DataFrame()\n",
    "        for one in self.__feature[:-1]:\n",
    "            col = self.__data[one]\n",
    "            if (str(list(col)[0]).split(\".\")[0]).isdigit() or str(list(col)[0]).isdigit() or (str(list(col)[0]).split('-')[-1]).split(\".\")[-1].isdigit():\n",
    "                new_data[one] = self.__data[one]\n",
    "                # print '%s 是数值型' % one\n",
    "            else:\n",
    "                # print '%s 是离散型' % one\n",
    "                keys = list(set(list(col)))\n",
    "                values = list(xrange(len(keys)))\n",
    "                new = dict(zip(keys, values))\n",
    "                new_data[one] = self.__data[one].map(new)\n",
    "        new_data[self.__feature[-1]] = self.__data[self.__feature[-1]]\n",
    "        return new_data\n",
    "\n",
    "    # 返回一个样本的k个猜中近邻和其他类的k个猜错近邻\n",
    "    def get_neighbors(self, row):\n",
    "        df = self.get_data()\n",
    "        row_type = row[df.columns[-1]]\n",
    "        right_df = df[df[df.columns[-1]] == row_type].drop(columns=[df.columns[-1]])\n",
    "        aim = row.drop(df.columns[-1])\n",
    "        f = lambda x: eulidSim(np.mat(x.values), np.mat(aim.values))\n",
    "        right_sim = right_df.apply(f, axis=1)\n",
    "        right_sim_two = right_sim.drop(right_sim.idxmin())\n",
    "        right = dict()\n",
    "        right[row_type] = list(right_sim_two.sort_values().index[0:self.__k])\n",
    "        # print list(right_sim_two.sort_values().index[0:self.__k])\n",
    "        types = list(set(df[df.columns[-1]]) - set([row_type]))\n",
    "        wrong = dict()\n",
    "        for one in types:\n",
    "            wrong_df = df[df[df.columns[-1]] == one].drop(columns=[df.columns[-1]])\n",
    "            wrong_sim = wrong_df.apply(f, axis=1)\n",
    "            wrong[one] = list(wrong_sim.sort_values().index[0:self.__k])\n",
    "        print( right, wrong)\n",
    "        return right, wrong\n",
    "\n",
    "    # 计算特征权重\n",
    "    def get_weight(self, feature, index, NearHit, NearMiss):\n",
    "        # data = self.__data.drop(self.__feature[-1], axis=1)\n",
    "        data = self.__data\n",
    "        row = data.iloc[index]\n",
    "        right = 0\n",
    "        for one in list(NearHit.values())[0]:\n",
    "            nearhit = data.iloc[one]\n",
    "            if (str(row[feature]).split(\".\")[0]).isdigit() or str(row[feature]).isdigit() or (str(row[feature]).split('-')[-1]).split(\".\")[-1].isdigit():\n",
    "                max_feature = data[feature].max()\n",
    "                min_feature = data[feature].min()\n",
    "                right_one = pow(round(abs(row[feature] - nearhit[feature]) / (max_feature - min_feature), 2), 2)\n",
    "            else:\n",
    "                right_one = 0 if row[feature] == nearhit[feature] else 1\n",
    "            right += right_one\n",
    "        right_w = round(right / self.__k, 2)\n",
    "\n",
    "        wrong_w = 0\n",
    "        # 样本row所在的种类占样本集的比例\n",
    "        p_row = round(float(list(data[data.columns[-1]]).count(row[data.columns[-1]])) / len(data), 2)\n",
    "        for one in NearMiss.keys():\n",
    "            # 种类one在样本集中所占的比例\n",
    "            p_one = round(float(list(data[data.columns[-1]]).count(one)) / len(data), 2)\n",
    "            wrong_one = 0\n",
    "            for i in NearMiss[one]:\n",
    "                nearmiss = data.iloc[i]\n",
    "                if (str(row[feature]).split(\".\")[0]).isdigit() or str(row[feature]).isdigit() or (str(row[feature]).split('-')[-1]).split(\".\")[-1].isdigit():\n",
    "                    max_feature = data[feature].max()\n",
    "                    min_feature = data[feature].min()\n",
    "                    wrong_one_one = pow(round(abs(row[feature] - nearmiss[feature]) / (max_feature - min_feature), 2), 2)\n",
    "                else:\n",
    "                    wrong_one_one = 0 if row[feature] == nearmiss[feature] else 1\n",
    "                wrong_one += wrong_one_one\n",
    "\n",
    "            wrong = round(p_one / (1 - p_row) * wrong_one / self.__k, 2)\n",
    "            wrong_w += wrong\n",
    "        w = wrong_w - right_w\n",
    "        return w\n",
    "\n",
    "    # 过滤式特征选择\n",
    "    def reliefF(self):\n",
    "        sample = self.get_data()\n",
    "        # print sample\n",
    "        m, n = np.shape(self.__data)  # m为行数，n为列数\n",
    "        score = []\n",
    "        sample_index = random.sample(range(0, m), self.__sample_num)\n",
    "        print ('采样样本索引为 %s ' % sample_index)\n",
    "        num = 1\n",
    "        for i in sample_index:    # 采样次数\n",
    "            one_score = dict()\n",
    "            row = sample.iloc[i]\n",
    "            NearHit, NearMiss = self.get_neighbors(row)\n",
    "            print( '第 %s 次采样，样本index为 %s，其NearHit k近邻行索引为 %s ，NearMiss k近邻行索引为 %s' % (num, i, NearHit, NearMiss))\n",
    "            for f in self.__feature[0:-1]:\n",
    "                w = self.get_weight(f, i, NearHit, NearMiss)\n",
    "                one_score[f] = w\n",
    "                print ('特征 %s 的权重为 %s.' % (f, w))\n",
    "            score.append(one_score)\n",
    "            num += 1\n",
    "        f_w = pd.DataFrame(score)\n",
    "        print( '采样各样本特征权重如下：')\n",
    "        print (f_w)\n",
    "        print( '平均特征权重如下：')\n",
    "        print (f_w.mean())\n",
    "        return f_w.mean()\n",
    "\n",
    "    # 返回最终选取的特征\n",
    "    def get_final(self):\n",
    "        f_w = pd.DataFrame(self.reliefF(), columns=['weight'])\n",
    "        final_feature_t = f_w[f_w['weight'] > self.__t]\n",
    "        print( final_feature_t)\n",
    "        # final_feature_k = f_w.sort_values('weight').head(self.__k)\n",
    "        # print final_feature_k\n",
    "        return final_feature_t\n",
    "\n",
    "\n",
    "# 几种距离求解\n",
    "def eulidSim(vecA, vecB):\n",
    "    return la.norm(vecA - vecB)\n",
    "\n",
    "\n",
    "def cosSim(vecA, vecB):\n",
    "    \"\"\"\n",
    "    :param vecA: 行向量\n",
    "    :param vecB: 行向量\n",
    "    :return: 返回余弦相似度（范围在0-1之间）\n",
    "    \"\"\"\n",
    "    num = float(vecA * vecB.T)\n",
    "    denom = la.norm(vecA) * la.norm(vecB)\n",
    "    cosSim = 0.5 + 0.5 * (num / denom)\n",
    "    return cosSim\n",
    "\n",
    "\n",
    "def pearsSim(vecA, vecB):\n",
    "    if len(vecA) < 3:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.5 + 0.5 * np.corrcoef(vecA, vecB, rowvar=0)[0][1]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = A\n",
    "    #data = np.ndarray.tolist(data)\n",
    "    print (data)\n",
    "    f = Relief(data, 1, 0.15, 2)\n",
    "    # df = f.get_data()\n",
    "    # print type(df.iloc[0])\n",
    "    # f.get_neighbors(df.iloc[0])\n",
    "    # f.get_weight('色泽', 6, 7, 8)\n",
    "    f.reliefF()\n",
    "    # f.get_final()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 降维后选择权重最大的14个指标进行训练\n",
    "A=X[['TypeOfSteel_A300','TypeOfSteel_A400','Outside_Global_Index','Length_of_Conveyer','Edges_Index','SigmoidOfAreas','Square_Index','Steel_Plate_Thickness','X_Minimum','X_Maximum','Orientation_Index','Edges_X_Index','Edges_Y_Index','Minimum_of_Luminosity','Log_X_Index','Y_Minimum','Y_Maximum','LogOfAreas','Empty_Index','Luminosity_Index','Maximum_of_Luminosity','Outside_X_Index','Log_Y_Index','Sum_of_Luminosity']]\n",
    "\n",
    "A\n",
    "\n",
    "pca = PCA(n_components = 12)\n",
    "\n",
    "# 利用数据训练模型（即上述得出特征向量的过程）\n",
    "pca.fit(X)\n",
    "\n",
    "# 得出原始数据的降维后的结果；也可以以新的数据作为参数，得到降维结果。\n",
    "print(pca.transform(X))\n",
    "B=pca.transform(X)\n",
    "# 打印各主成分的方差占比\n",
    "print(pca.explained_variance_ratio_)\n",
    "data_after_PCA=pd.DataFrame(B)\n",
    "data_after_PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.75482336\n",
      "Iteration 2, loss = 1.34620904\n",
      "Iteration 3, loss = 1.09243051\n",
      "Iteration 4, loss = 0.92692349\n",
      "Iteration 5, loss = 0.81264387\n",
      "Iteration 6, loss = 0.74851518\n",
      "Iteration 7, loss = 0.70004489\n",
      "Iteration 8, loss = 0.65719106\n",
      "Iteration 9, loss = 0.62873699\n",
      "Iteration 10, loss = 0.60932346\n",
      "Iteration 11, loss = 0.58645490\n",
      "Iteration 12, loss = 0.56772341\n",
      "Iteration 13, loss = 0.54868262\n",
      "Iteration 14, loss = 0.53076156\n",
      "Iteration 15, loss = 0.51855343\n",
      "Iteration 16, loss = 0.50394732\n",
      "Iteration 17, loss = 0.49044617\n",
      "Iteration 18, loss = 0.47807081\n",
      "Iteration 19, loss = 0.47028534\n",
      "Iteration 20, loss = 0.46115355\n",
      "Iteration 21, loss = 0.45296211\n",
      "Iteration 22, loss = 0.44716902\n",
      "Iteration 23, loss = 0.43162022\n",
      "Iteration 24, loss = 0.42598229\n",
      "Iteration 25, loss = 0.41799096\n",
      "Iteration 26, loss = 0.40487414\n",
      "Iteration 27, loss = 0.40146645\n",
      "Iteration 28, loss = 0.39550112\n",
      "Iteration 29, loss = 0.38824003\n",
      "Iteration 30, loss = 0.37708649\n",
      "Iteration 31, loss = 0.36480131\n",
      "Iteration 32, loss = 0.35983946\n",
      "Iteration 33, loss = 0.35696216\n",
      "Iteration 34, loss = 0.34773523\n",
      "Iteration 35, loss = 0.33945530\n",
      "Iteration 36, loss = 0.33352750\n",
      "Iteration 37, loss = 0.32981912\n",
      "Iteration 38, loss = 0.32127481\n",
      "Iteration 39, loss = 0.31712612\n",
      "Iteration 40, loss = 0.30753524\n",
      "Iteration 41, loss = 0.30346915\n",
      "Iteration 42, loss = 0.29755575\n",
      "Iteration 43, loss = 0.29479071\n",
      "Iteration 44, loss = 0.28455160\n",
      "Iteration 45, loss = 0.27812313\n",
      "Iteration 46, loss = 0.27386527\n",
      "Iteration 47, loss = 0.27155956\n",
      "Iteration 48, loss = 0.26792165\n",
      "Iteration 49, loss = 0.26529947\n",
      "Iteration 50, loss = 0.26062529\n",
      "Iteration 51, loss = 0.25547566\n",
      "Iteration 52, loss = 0.24500132\n",
      "Iteration 53, loss = 0.23931685\n",
      "Iteration 54, loss = 0.22947553\n",
      "Iteration 55, loss = 0.22720032\n",
      "Iteration 56, loss = 0.22846712\n",
      "Iteration 57, loss = 0.22170630\n",
      "Iteration 58, loss = 0.21725548\n",
      "Iteration 59, loss = 0.21266697\n",
      "Iteration 60, loss = 0.20713168\n",
      "Iteration 61, loss = 0.20834750\n",
      "Iteration 62, loss = 0.19751484\n",
      "Iteration 63, loss = 0.19532381\n",
      "Iteration 64, loss = 0.19255690\n",
      "Iteration 65, loss = 0.18436622\n",
      "Iteration 66, loss = 0.18256489\n",
      "Iteration 67, loss = 0.17964454\n",
      "Iteration 68, loss = 0.17224123\n",
      "Iteration 69, loss = 0.16942028\n",
      "Iteration 70, loss = 0.16537920\n",
      "Iteration 71, loss = 0.16519610\n",
      "Iteration 72, loss = 0.17124602\n",
      "Iteration 73, loss = 0.15706181\n",
      "Iteration 74, loss = 0.15725284\n",
      "Iteration 75, loss = 0.15620159\n",
      "Iteration 76, loss = 0.15682851\n",
      "Iteration 77, loss = 0.15239276\n",
      "Iteration 78, loss = 0.16319580\n",
      "Iteration 79, loss = 0.14221850\n",
      "Iteration 80, loss = 0.13829055\n",
      "Iteration 81, loss = 0.13797811\n",
      "Iteration 82, loss = 0.14411921\n",
      "Iteration 83, loss = 0.14000881\n",
      "Iteration 84, loss = 0.13880036\n",
      "Iteration 85, loss = 0.13654403\n",
      "Iteration 86, loss = 0.12249308\n",
      "Iteration 87, loss = 0.11421337\n",
      "Iteration 88, loss = 0.11670662\n",
      "Iteration 89, loss = 0.10996163\n",
      "Iteration 90, loss = 0.10392963\n",
      "Iteration 91, loss = 0.10167490\n",
      "Iteration 92, loss = 0.10145019\n",
      "Iteration 93, loss = 0.10531260\n",
      "Iteration 94, loss = 0.10581596\n",
      "Iteration 95, loss = 0.10254680\n",
      "Iteration 96, loss = 0.09520919\n",
      "Iteration 97, loss = 0.10000738\n",
      "Iteration 98, loss = 0.10157206\n",
      "Iteration 99, loss = 0.09828559\n",
      "Iteration 100, loss = 0.09081109\n",
      "Iteration 101, loss = 0.08453456\n",
      "Iteration 102, loss = 0.08290284\n",
      "Iteration 103, loss = 0.08035853\n",
      "Iteration 104, loss = 0.08510309\n",
      "Iteration 105, loss = 0.08137913\n",
      "Iteration 106, loss = 0.07866299\n",
      "Iteration 107, loss = 0.07427442\n",
      "Iteration 108, loss = 0.07294002\n",
      "Iteration 109, loss = 0.06825882\n",
      "Iteration 110, loss = 0.06759499\n",
      "Iteration 111, loss = 0.07048651\n",
      "Iteration 112, loss = 0.07318417\n",
      "Iteration 113, loss = 0.07235981\n",
      "Iteration 114, loss = 0.06691226\n",
      "Iteration 115, loss = 0.06273896\n",
      "Iteration 116, loss = 0.06329077\n",
      "Iteration 117, loss = 0.05948480\n",
      "Iteration 118, loss = 0.05819261\n",
      "Iteration 119, loss = 0.06027174\n",
      "Iteration 120, loss = 0.05842748\n",
      "Iteration 121, loss = 0.05999164\n",
      "Iteration 122, loss = 0.05608135\n",
      "Iteration 123, loss = 0.05205990\n",
      "Iteration 124, loss = 0.05261378\n",
      "Iteration 125, loss = 0.05224941\n",
      "Iteration 126, loss = 0.05235412\n",
      "Iteration 127, loss = 0.05086064\n",
      "Iteration 128, loss = 0.04760796\n",
      "Iteration 129, loss = 0.04712511\n",
      "Iteration 130, loss = 0.04559316\n",
      "Iteration 131, loss = 0.04693066\n",
      "Iteration 132, loss = 0.04942871\n",
      "Iteration 133, loss = 0.04264525\n",
      "Iteration 134, loss = 0.04113402\n",
      "Iteration 135, loss = 0.04030133\n",
      "Iteration 136, loss = 0.03858097\n",
      "Iteration 137, loss = 0.03856364\n",
      "Iteration 138, loss = 0.03744009\n",
      "Iteration 139, loss = 0.03704131\n",
      "Iteration 140, loss = 0.03589024\n",
      "Iteration 141, loss = 0.03619561\n",
      "Iteration 142, loss = 0.03883517\n",
      "Iteration 143, loss = 0.04007163\n",
      "Iteration 144, loss = 0.03927363\n",
      "Iteration 145, loss = 0.03677986\n",
      "Iteration 146, loss = 0.03406610\n",
      "Iteration 147, loss = 0.03558096\n",
      "Iteration 148, loss = 0.03376371\n",
      "Iteration 149, loss = 0.03200555\n",
      "Iteration 150, loss = 0.03204596\n",
      "Iteration 151, loss = 0.03079792\n",
      "Iteration 152, loss = 0.03023677\n",
      "Iteration 153, loss = 0.03293893\n",
      "Iteration 154, loss = 0.03045028\n",
      "Iteration 155, loss = 0.02600854\n",
      "Iteration 156, loss = 0.02555890\n",
      "Iteration 157, loss = 0.02508924\n",
      "Iteration 158, loss = 0.02465396\n",
      "Iteration 159, loss = 0.02653458\n",
      "Iteration 160, loss = 0.02721857\n",
      "Iteration 161, loss = 0.02429821\n",
      "Iteration 162, loss = 0.02403161\n",
      "Iteration 163, loss = 0.02380572\n",
      "Iteration 164, loss = 0.02607644\n",
      "Iteration 165, loss = 0.03192751\n",
      "Iteration 166, loss = 0.03004693\n",
      "Iteration 167, loss = 0.03007615\n",
      "Iteration 168, loss = 0.03048998\n",
      "Iteration 169, loss = 0.02606609\n",
      "Iteration 170, loss = 0.02428893\n",
      "Iteration 171, loss = 0.02709821\n",
      "Iteration 172, loss = 0.02505092\n",
      "Iteration 173, loss = 0.02129662\n",
      "Iteration 174, loss = 0.02258423\n",
      "Iteration 175, loss = 0.01925812\n",
      "Iteration 176, loss = 0.01838805\n",
      "Iteration 177, loss = 0.01812374\n",
      "Iteration 178, loss = 0.02298438\n",
      "Iteration 179, loss = 0.02173085\n",
      "Iteration 180, loss = 0.02141864\n",
      "Iteration 181, loss = 0.01795792\n",
      "Iteration 182, loss = 0.02120307\n",
      "Iteration 183, loss = 0.01830938\n",
      "Iteration 184, loss = 0.01618146\n",
      "Iteration 185, loss = 0.01829290\n",
      "Iteration 186, loss = 0.01455448\n",
      "Iteration 187, loss = 0.01693603\n",
      "Iteration 188, loss = 0.01532735\n",
      "Iteration 189, loss = 0.01469590\n",
      "Iteration 190, loss = 0.01413767\n",
      "Iteration 191, loss = 0.01877671\n",
      "Iteration 192, loss = 0.01554775\n",
      "Iteration 193, loss = 0.01352641\n",
      "Iteration 194, loss = 0.01277539\n",
      "Iteration 195, loss = 0.01326287\n",
      "Iteration 196, loss = 0.01413943\n",
      "Iteration 197, loss = 0.01257517\n",
      "Iteration 198, loss = 0.01414923\n",
      "Iteration 199, loss = 0.01370156\n",
      "Iteration 200, loss = 0.01400344\n",
      "Iteration 201, loss = 0.01305957\n",
      "Iteration 202, loss = 0.01499509\n",
      "Iteration 203, loss = 0.01428617\n",
      "Iteration 204, loss = 0.01405004\n",
      "Iteration 205, loss = 0.01157633\n",
      "Iteration 206, loss = 0.01382605\n",
      "Iteration 207, loss = 0.01454698\n",
      "Iteration 208, loss = 0.01618006\n",
      "Iteration 209, loss = 0.02265225\n",
      "Iteration 210, loss = 0.01556943\n",
      "Iteration 211, loss = 0.01283797\n",
      "Iteration 212, loss = 0.01355878\n",
      "Iteration 213, loss = 0.01187621\n",
      "Iteration 214, loss = 0.01054367\n",
      "Iteration 215, loss = 0.01168390\n",
      "Iteration 216, loss = 0.00985332\n",
      "Iteration 217, loss = 0.01103182\n",
      "Iteration 218, loss = 0.01137185\n",
      "Iteration 219, loss = 0.00950416\n",
      "Iteration 220, loss = 0.01051225\n",
      "Iteration 221, loss = 0.00925477\n",
      "Iteration 222, loss = 0.00937801\n",
      "Iteration 223, loss = 0.00902286\n",
      "Iteration 224, loss = 0.01062989\n",
      "Iteration 225, loss = 0.01680172\n",
      "Iteration 226, loss = 0.01160299\n",
      "Iteration 227, loss = 0.01308185\n",
      "Iteration 228, loss = 0.00914227\n",
      "Iteration 229, loss = 0.00960883\n",
      "Iteration 230, loss = 0.00811038\n",
      "Iteration 231, loss = 0.00833318\n",
      "Iteration 232, loss = 0.00933660\n",
      "Iteration 233, loss = 0.01096633\n",
      "Iteration 234, loss = 0.00806991\n",
      "Iteration 235, loss = 0.00787125\n",
      "Iteration 236, loss = 0.00907954\n",
      "Iteration 237, loss = 0.00734374\n",
      "Iteration 238, loss = 0.00898048\n",
      "Iteration 239, loss = 0.00750426\n",
      "Iteration 240, loss = 0.00865994\n",
      "Iteration 241, loss = 0.00735089\n",
      "Iteration 242, loss = 0.01349886\n",
      "Iteration 243, loss = 0.00833726\n",
      "Iteration 244, loss = 0.00974487\n",
      "Iteration 245, loss = 0.00859365\n",
      "Iteration 246, loss = 0.01420709\n",
      "Iteration 247, loss = 0.00779637\n",
      "Iteration 248, loss = 0.00803728\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.74387442\n",
      "Iteration 2, loss = 1.31118663\n",
      "Iteration 3, loss = 1.08145815\n",
      "Iteration 4, loss = 0.93428880\n",
      "Iteration 5, loss = 0.82458888\n",
      "Iteration 6, loss = 0.75197713\n",
      "Iteration 7, loss = 0.69975970\n",
      "Iteration 8, loss = 0.65868687\n",
      "Iteration 9, loss = 0.62890871\n",
      "Iteration 10, loss = 0.60776465\n",
      "Iteration 11, loss = 0.58495610\n",
      "Iteration 12, loss = 0.56272278\n",
      "Iteration 13, loss = 0.54998926\n",
      "Iteration 14, loss = 0.53571705\n",
      "Iteration 15, loss = 0.52057635\n",
      "Iteration 16, loss = 0.50785213\n",
      "Iteration 17, loss = 0.49403835\n",
      "Iteration 18, loss = 0.48623728\n",
      "Iteration 19, loss = 0.47672853\n",
      "Iteration 20, loss = 0.46563001\n",
      "Iteration 21, loss = 0.45233733\n",
      "Iteration 22, loss = 0.44281477\n",
      "Iteration 23, loss = 0.43306065\n",
      "Iteration 24, loss = 0.42257976\n",
      "Iteration 25, loss = 0.41728187\n",
      "Iteration 26, loss = 0.40530324\n",
      "Iteration 27, loss = 0.40156546\n",
      "Iteration 28, loss = 0.38990299\n",
      "Iteration 29, loss = 0.38225575\n",
      "Iteration 30, loss = 0.37932320\n",
      "Iteration 31, loss = 0.36288584\n",
      "Iteration 32, loss = 0.35901708\n",
      "Iteration 33, loss = 0.34752982\n",
      "Iteration 34, loss = 0.34822875\n",
      "Iteration 35, loss = 0.33853101\n",
      "Iteration 36, loss = 0.33017400\n",
      "Iteration 37, loss = 0.32235430\n",
      "Iteration 38, loss = 0.31661301\n",
      "Iteration 39, loss = 0.30620351\n",
      "Iteration 40, loss = 0.30559707\n",
      "Iteration 41, loss = 0.29597587\n",
      "Iteration 42, loss = 0.29611451\n",
      "Iteration 43, loss = 0.28424240\n",
      "Iteration 44, loss = 0.27634082\n",
      "Iteration 45, loss = 0.27023310\n",
      "Iteration 46, loss = 0.26444550\n",
      "Iteration 47, loss = 0.25802240\n",
      "Iteration 48, loss = 0.25461393\n",
      "Iteration 49, loss = 0.24705563\n",
      "Iteration 50, loss = 0.23813380\n",
      "Iteration 51, loss = 0.23188079\n",
      "Iteration 52, loss = 0.22505976\n",
      "Iteration 53, loss = 0.22295612\n",
      "Iteration 54, loss = 0.21714432\n",
      "Iteration 55, loss = 0.21144071\n",
      "Iteration 56, loss = 0.20720176\n",
      "Iteration 57, loss = 0.20145399\n",
      "Iteration 58, loss = 0.19820815\n",
      "Iteration 59, loss = 0.20105941\n",
      "Iteration 60, loss = 0.19353105\n",
      "Iteration 61, loss = 0.18106829\n",
      "Iteration 62, loss = 0.17553952\n",
      "Iteration 63, loss = 0.16954338\n",
      "Iteration 64, loss = 0.17085434\n",
      "Iteration 65, loss = 0.16453812\n",
      "Iteration 66, loss = 0.16532376\n",
      "Iteration 67, loss = 0.15736635\n",
      "Iteration 68, loss = 0.15599503\n",
      "Iteration 69, loss = 0.15481521\n",
      "Iteration 70, loss = 0.14420461\n",
      "Iteration 71, loss = 0.13920775\n",
      "Iteration 72, loss = 0.13555716\n",
      "Iteration 73, loss = 0.13464466\n",
      "Iteration 74, loss = 0.13346577\n",
      "Iteration 75, loss = 0.13079825\n",
      "Iteration 76, loss = 0.12069788\n",
      "Iteration 77, loss = 0.12129981\n",
      "Iteration 78, loss = 0.11902401\n",
      "Iteration 79, loss = 0.11612191\n",
      "Iteration 80, loss = 0.11677642\n",
      "Iteration 81, loss = 0.11470927\n",
      "Iteration 82, loss = 0.11045092\n",
      "Iteration 83, loss = 0.10500570\n",
      "Iteration 84, loss = 0.10395819\n",
      "Iteration 85, loss = 0.10333205\n",
      "Iteration 86, loss = 0.09497391\n",
      "Iteration 87, loss = 0.09634644\n",
      "Iteration 88, loss = 0.10097978\n",
      "Iteration 89, loss = 0.09364062\n",
      "Iteration 90, loss = 0.08815417\n",
      "Iteration 91, loss = 0.08406321\n",
      "Iteration 92, loss = 0.08061934\n",
      "Iteration 93, loss = 0.07969576\n",
      "Iteration 94, loss = 0.07680293\n",
      "Iteration 95, loss = 0.07567907\n",
      "Iteration 96, loss = 0.07214174\n",
      "Iteration 97, loss = 0.07251719\n",
      "Iteration 98, loss = 0.07651708\n",
      "Iteration 99, loss = 0.08101464\n",
      "Iteration 100, loss = 0.07777699\n",
      "Iteration 101, loss = 0.07091092\n",
      "Iteration 102, loss = 0.06900417\n",
      "Iteration 103, loss = 0.06236812\n",
      "Iteration 104, loss = 0.06198208\n",
      "Iteration 105, loss = 0.05837485\n",
      "Iteration 106, loss = 0.05650205\n",
      "Iteration 107, loss = 0.05547171\n",
      "Iteration 108, loss = 0.05250481\n",
      "Iteration 109, loss = 0.05364128\n",
      "Iteration 110, loss = 0.05117648\n",
      "Iteration 111, loss = 0.05262593\n",
      "Iteration 112, loss = 0.05406273\n",
      "Iteration 113, loss = 0.04697026\n",
      "Iteration 114, loss = 0.04573181\n",
      "Iteration 115, loss = 0.04376698\n",
      "Iteration 116, loss = 0.04259977\n",
      "Iteration 117, loss = 0.04200324\n",
      "Iteration 118, loss = 0.04277581\n",
      "Iteration 119, loss = 0.04388489\n",
      "Iteration 120, loss = 0.04076389\n",
      "Iteration 121, loss = 0.03993487\n",
      "Iteration 122, loss = 0.04007662\n",
      "Iteration 123, loss = 0.03827248\n",
      "Iteration 124, loss = 0.03755787\n",
      "Iteration 125, loss = 0.03382345\n",
      "Iteration 126, loss = 0.03466349\n",
      "Iteration 127, loss = 0.03389870\n",
      "Iteration 128, loss = 0.03423464\n",
      "Iteration 129, loss = 0.03367918\n",
      "Iteration 130, loss = 0.03870763\n",
      "Iteration 131, loss = 0.03185134\n",
      "Iteration 132, loss = 0.03089174\n",
      "Iteration 133, loss = 0.02747576\n",
      "Iteration 134, loss = 0.02753276\n",
      "Iteration 135, loss = 0.02633735\n",
      "Iteration 136, loss = 0.02721843\n",
      "Iteration 137, loss = 0.02755382\n",
      "Iteration 138, loss = 0.02760372\n",
      "Iteration 139, loss = 0.02573067\n",
      "Iteration 140, loss = 0.02456703\n",
      "Iteration 141, loss = 0.02517026\n",
      "Iteration 142, loss = 0.03091075\n",
      "Iteration 143, loss = 0.03532047\n",
      "Iteration 144, loss = 0.03054745\n",
      "Iteration 145, loss = 0.02370255\n",
      "Iteration 146, loss = 0.02435808\n",
      "Iteration 147, loss = 0.02048878\n",
      "Iteration 148, loss = 0.02109620\n",
      "Iteration 149, loss = 0.02009971\n",
      "Iteration 150, loss = 0.02036167\n",
      "Iteration 151, loss = 0.02114028\n",
      "Iteration 152, loss = 0.01854317\n",
      "Iteration 153, loss = 0.02119216\n",
      "Iteration 154, loss = 0.02222975\n",
      "Iteration 155, loss = 0.01986078\n",
      "Iteration 156, loss = 0.02049817\n",
      "Iteration 157, loss = 0.01893740\n",
      "Iteration 158, loss = 0.01796203\n",
      "Iteration 159, loss = 0.01706387\n",
      "Iteration 160, loss = 0.01724677\n",
      "Iteration 161, loss = 0.01667019\n",
      "Iteration 162, loss = 0.01617323\n",
      "Iteration 163, loss = 0.01787051\n",
      "Iteration 164, loss = 0.01746228\n",
      "Iteration 165, loss = 0.02087284\n",
      "Iteration 166, loss = 0.01695252\n",
      "Iteration 167, loss = 0.01475635\n",
      "Iteration 168, loss = 0.01468094\n",
      "Iteration 169, loss = 0.01450268\n",
      "Iteration 170, loss = 0.01414916\n",
      "Iteration 171, loss = 0.01590857\n",
      "Iteration 172, loss = 0.01551900\n",
      "Iteration 173, loss = 0.01380503\n",
      "Iteration 174, loss = 0.01278092\n",
      "Iteration 175, loss = 0.01321027\n",
      "Iteration 176, loss = 0.01517799\n",
      "Iteration 177, loss = 0.01388275\n",
      "Iteration 178, loss = 0.01452544\n",
      "Iteration 179, loss = 0.01666007\n",
      "Iteration 180, loss = 0.01550430\n",
      "Iteration 181, loss = 0.01504706\n",
      "Iteration 182, loss = 0.01323609\n",
      "Iteration 183, loss = 0.01150819\n",
      "Iteration 184, loss = 0.01238531\n",
      "Iteration 185, loss = 0.01100752\n",
      "Iteration 186, loss = 0.01140816\n",
      "Iteration 187, loss = 0.01067745\n",
      "Iteration 188, loss = 0.01121562\n",
      "Iteration 189, loss = 0.01283720\n",
      "Iteration 190, loss = 0.00981292\n",
      "Iteration 191, loss = 0.01051943\n",
      "Iteration 192, loss = 0.01207307\n",
      "Iteration 193, loss = 0.00933924\n",
      "Iteration 194, loss = 0.00914664\n",
      "Iteration 195, loss = 0.00883164\n",
      "Iteration 196, loss = 0.00865674\n",
      "Iteration 197, loss = 0.01108783\n",
      "Iteration 198, loss = 0.00858075\n",
      "Iteration 199, loss = 0.00917135\n",
      "Iteration 200, loss = 0.00912489\n",
      "Iteration 201, loss = 0.01077842\n",
      "Iteration 202, loss = 0.01643662\n",
      "Iteration 203, loss = 0.01046752\n",
      "Iteration 204, loss = 0.00989199\n",
      "Iteration 205, loss = 0.01193065\n",
      "Iteration 206, loss = 0.00972258\n",
      "Iteration 207, loss = 0.00856232\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.80214651\n",
      "Iteration 2, loss = 1.35399185\n",
      "Iteration 3, loss = 1.10999636\n",
      "Iteration 4, loss = 0.95122675\n",
      "Iteration 5, loss = 0.84916189\n",
      "Iteration 6, loss = 0.78054082\n",
      "Iteration 7, loss = 0.72604064\n",
      "Iteration 8, loss = 0.68822349\n",
      "Iteration 9, loss = 0.65679873\n",
      "Iteration 10, loss = 0.62898741\n",
      "Iteration 11, loss = 0.61015877\n",
      "Iteration 12, loss = 0.58863677\n",
      "Iteration 13, loss = 0.56788538\n",
      "Iteration 14, loss = 0.54661566\n",
      "Iteration 15, loss = 0.53212276\n",
      "Iteration 16, loss = 0.51922513\n",
      "Iteration 17, loss = 0.51387013\n",
      "Iteration 18, loss = 0.49587894\n",
      "Iteration 19, loss = 0.48178593\n",
      "Iteration 20, loss = 0.47278849\n",
      "Iteration 21, loss = 0.46011672\n",
      "Iteration 22, loss = 0.45237144\n",
      "Iteration 23, loss = 0.44430601\n",
      "Iteration 24, loss = 0.44832284\n",
      "Iteration 25, loss = 0.43362588\n",
      "Iteration 26, loss = 0.42671933\n",
      "Iteration 27, loss = 0.40993194\n",
      "Iteration 28, loss = 0.39993430\n",
      "Iteration 29, loss = 0.39173052\n",
      "Iteration 30, loss = 0.38375519\n",
      "Iteration 31, loss = 0.37775560\n",
      "Iteration 32, loss = 0.36920077\n",
      "Iteration 33, loss = 0.36331692\n",
      "Iteration 34, loss = 0.36031916\n",
      "Iteration 35, loss = 0.35238113\n",
      "Iteration 36, loss = 0.34413936\n",
      "Iteration 37, loss = 0.33445260\n",
      "Iteration 38, loss = 0.33392997\n",
      "Iteration 39, loss = 0.32659734\n",
      "Iteration 40, loss = 0.32080127\n",
      "Iteration 41, loss = 0.31049824\n",
      "Iteration 42, loss = 0.30287222\n",
      "Iteration 43, loss = 0.29995854\n",
      "Iteration 44, loss = 0.29495790\n",
      "Iteration 45, loss = 0.28635743\n",
      "Iteration 46, loss = 0.28150144\n",
      "Iteration 47, loss = 0.27170379\n",
      "Iteration 48, loss = 0.27053424\n",
      "Iteration 49, loss = 0.26318602\n",
      "Iteration 50, loss = 0.26194621\n",
      "Iteration 51, loss = 0.25464489\n",
      "Iteration 52, loss = 0.25628008\n",
      "Iteration 53, loss = 0.24587158\n",
      "Iteration 54, loss = 0.24061744\n",
      "Iteration 55, loss = 0.23873214\n",
      "Iteration 56, loss = 0.22684761\n",
      "Iteration 57, loss = 0.21678287\n",
      "Iteration 58, loss = 0.21711083\n",
      "Iteration 59, loss = 0.22008322\n",
      "Iteration 60, loss = 0.21778198\n",
      "Iteration 61, loss = 0.20888368\n",
      "Iteration 62, loss = 0.19929507\n",
      "Iteration 63, loss = 0.19237813\n",
      "Iteration 64, loss = 0.19110587\n",
      "Iteration 65, loss = 0.19605976\n",
      "Iteration 66, loss = 0.18862596\n",
      "Iteration 67, loss = 0.18071770\n",
      "Iteration 68, loss = 0.17598006\n",
      "Iteration 69, loss = 0.16636180\n",
      "Iteration 70, loss = 0.16314102\n",
      "Iteration 71, loss = 0.15794211\n",
      "Iteration 72, loss = 0.15513183\n",
      "Iteration 73, loss = 0.15277862\n",
      "Iteration 74, loss = 0.15287266\n",
      "Iteration 75, loss = 0.14826952\n",
      "Iteration 76, loss = 0.14641209\n",
      "Iteration 77, loss = 0.13602582\n",
      "Iteration 78, loss = 0.13115994\n",
      "Iteration 79, loss = 0.13591328\n",
      "Iteration 80, loss = 0.13286580\n",
      "Iteration 81, loss = 0.12687652\n",
      "Iteration 82, loss = 0.12214889\n",
      "Iteration 83, loss = 0.12357508\n",
      "Iteration 84, loss = 0.12217170\n",
      "Iteration 85, loss = 0.11504507\n",
      "Iteration 86, loss = 0.11290384\n",
      "Iteration 87, loss = 0.11013193\n",
      "Iteration 88, loss = 0.10261337\n",
      "Iteration 89, loss = 0.10130823\n",
      "Iteration 90, loss = 0.09843002\n",
      "Iteration 91, loss = 0.10458081\n",
      "Iteration 92, loss = 0.09657741\n",
      "Iteration 93, loss = 0.09445249\n",
      "Iteration 94, loss = 0.09540508\n",
      "Iteration 95, loss = 0.08971433\n",
      "Iteration 96, loss = 0.08528000\n",
      "Iteration 97, loss = 0.08991305\n",
      "Iteration 98, loss = 0.08380751\n",
      "Iteration 99, loss = 0.08314693\n",
      "Iteration 100, loss = 0.07894771\n",
      "Iteration 101, loss = 0.07619791\n",
      "Iteration 102, loss = 0.07379981\n",
      "Iteration 103, loss = 0.07079008\n",
      "Iteration 104, loss = 0.06817948\n",
      "Iteration 105, loss = 0.06742849\n",
      "Iteration 106, loss = 0.06389686\n",
      "Iteration 107, loss = 0.06907588\n",
      "Iteration 108, loss = 0.06602042\n",
      "Iteration 109, loss = 0.06495440\n",
      "Iteration 110, loss = 0.06217357\n",
      "Iteration 111, loss = 0.06156983\n",
      "Iteration 112, loss = 0.06353225\n",
      "Iteration 113, loss = 0.05719256\n",
      "Iteration 114, loss = 0.05441986\n",
      "Iteration 115, loss = 0.05472401\n",
      "Iteration 116, loss = 0.05263786\n",
      "Iteration 117, loss = 0.05364581\n",
      "Iteration 118, loss = 0.05656669\n",
      "Iteration 119, loss = 0.05320022\n",
      "Iteration 120, loss = 0.04530104\n",
      "Iteration 121, loss = 0.04355507\n",
      "Iteration 122, loss = 0.04449608\n",
      "Iteration 123, loss = 0.04121923\n",
      "Iteration 124, loss = 0.04429767\n",
      "Iteration 125, loss = 0.04684507\n",
      "Iteration 126, loss = 0.03997195\n",
      "Iteration 127, loss = 0.04200229\n",
      "Iteration 128, loss = 0.04105319\n",
      "Iteration 129, loss = 0.03942955\n",
      "Iteration 130, loss = 0.03504766\n",
      "Iteration 131, loss = 0.03791520\n",
      "Iteration 132, loss = 0.04412470\n",
      "Iteration 133, loss = 0.03720072\n",
      "Iteration 134, loss = 0.03802404\n",
      "Iteration 135, loss = 0.03476718\n",
      "Iteration 136, loss = 0.03593211\n",
      "Iteration 137, loss = 0.03915720\n",
      "Iteration 138, loss = 0.03633091\n",
      "Iteration 139, loss = 0.03450215\n",
      "Iteration 140, loss = 0.03263335\n",
      "Iteration 141, loss = 0.03434065\n",
      "Iteration 142, loss = 0.03179345\n",
      "Iteration 143, loss = 0.03275682\n",
      "Iteration 144, loss = 0.02797981\n",
      "Iteration 145, loss = 0.02666105\n",
      "Iteration 146, loss = 0.02621975\n",
      "Iteration 147, loss = 0.02925096\n",
      "Iteration 148, loss = 0.02778364\n",
      "Iteration 149, loss = 0.02782261\n",
      "Iteration 150, loss = 0.02824648\n",
      "Iteration 151, loss = 0.02844867\n",
      "Iteration 152, loss = 0.02718353\n",
      "Iteration 153, loss = 0.02829462\n",
      "Iteration 154, loss = 0.02874823\n",
      "Iteration 155, loss = 0.02571103\n",
      "Iteration 156, loss = 0.02301157\n",
      "Iteration 157, loss = 0.02142677\n",
      "Iteration 158, loss = 0.02132535\n",
      "Iteration 159, loss = 0.02189101\n",
      "Iteration 160, loss = 0.02138507\n",
      "Iteration 161, loss = 0.02291804\n",
      "Iteration 162, loss = 0.02080966\n",
      "Iteration 163, loss = 0.02019405\n",
      "Iteration 164, loss = 0.01967214\n",
      "Iteration 165, loss = 0.02060664\n",
      "Iteration 166, loss = 0.02065952\n",
      "Iteration 167, loss = 0.02112676\n",
      "Iteration 168, loss = 0.02041626\n",
      "Iteration 169, loss = 0.02212244\n",
      "Iteration 170, loss = 0.02145630\n",
      "Iteration 171, loss = 0.02105288\n",
      "Iteration 172, loss = 0.01956012\n",
      "Iteration 173, loss = 0.01936996\n",
      "Iteration 174, loss = 0.01748285\n",
      "Iteration 175, loss = 0.01593175\n",
      "Iteration 176, loss = 0.01470082\n",
      "Iteration 177, loss = 0.01454028\n",
      "Iteration 178, loss = 0.01414817\n",
      "Iteration 179, loss = 0.01371410\n",
      "Iteration 180, loss = 0.01488697\n",
      "Iteration 181, loss = 0.01560391\n",
      "Iteration 182, loss = 0.01354880\n",
      "Iteration 183, loss = 0.01436974\n",
      "Iteration 184, loss = 0.01479731\n",
      "Iteration 185, loss = 0.01771344\n",
      "Iteration 186, loss = 0.01779771\n",
      "Iteration 187, loss = 0.01651550\n",
      "Iteration 188, loss = 0.01407439\n",
      "Iteration 189, loss = 0.01384604\n",
      "Iteration 190, loss = 0.01226828\n",
      "Iteration 191, loss = 0.01359934\n",
      "Iteration 192, loss = 0.01198355\n",
      "Iteration 193, loss = 0.01118268\n",
      "Iteration 194, loss = 0.01158309\n",
      "Iteration 195, loss = 0.01159679\n",
      "Iteration 196, loss = 0.01735908\n",
      "Iteration 197, loss = 0.01380659\n",
      "Iteration 198, loss = 0.01281103\n",
      "Iteration 199, loss = 0.01274622\n",
      "Iteration 200, loss = 0.01204757\n",
      "Iteration 201, loss = 0.01184605\n",
      "Iteration 202, loss = 0.01124182\n",
      "Iteration 203, loss = 0.01089238\n",
      "Iteration 204, loss = 0.01008842\n",
      "Iteration 205, loss = 0.01221345\n",
      "Iteration 206, loss = 0.01178850\n",
      "Iteration 207, loss = 0.01386482\n",
      "Iteration 208, loss = 0.01135415\n",
      "Iteration 209, loss = 0.01306007\n",
      "Iteration 210, loss = 0.01361706\n",
      "Iteration 211, loss = 0.01168781\n",
      "Iteration 212, loss = 0.01176608\n",
      "Iteration 213, loss = 0.01289729\n",
      "Iteration 214, loss = 0.01150024\n",
      "Iteration 215, loss = 0.01207584\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.67872147\n",
      "Iteration 2, loss = 1.27381684\n",
      "Iteration 3, loss = 1.08541126\n",
      "Iteration 4, loss = 0.94977144\n",
      "Iteration 5, loss = 0.84133756\n",
      "Iteration 6, loss = 0.76385634\n",
      "Iteration 7, loss = 0.71166495\n",
      "Iteration 8, loss = 0.66982625\n",
      "Iteration 9, loss = 0.63875968\n",
      "Iteration 10, loss = 0.61211256\n",
      "Iteration 11, loss = 0.58669449\n",
      "Iteration 12, loss = 0.57462748\n",
      "Iteration 13, loss = 0.54645058\n",
      "Iteration 14, loss = 0.53087784\n",
      "Iteration 15, loss = 0.51605761\n",
      "Iteration 16, loss = 0.50431406\n",
      "Iteration 17, loss = 0.49686864\n",
      "Iteration 18, loss = 0.48547266\n",
      "Iteration 19, loss = 0.47180104\n",
      "Iteration 20, loss = 0.46165120\n",
      "Iteration 21, loss = 0.45530909\n",
      "Iteration 22, loss = 0.44623085\n",
      "Iteration 23, loss = 0.43912595\n",
      "Iteration 24, loss = 0.42452476\n",
      "Iteration 25, loss = 0.41613010\n",
      "Iteration 26, loss = 0.40775248\n",
      "Iteration 27, loss = 0.40482508\n",
      "Iteration 28, loss = 0.39758436\n",
      "Iteration 29, loss = 0.39591740\n",
      "Iteration 30, loss = 0.37999652\n",
      "Iteration 31, loss = 0.37117387\n",
      "Iteration 32, loss = 0.36167529\n",
      "Iteration 33, loss = 0.36871740\n",
      "Iteration 34, loss = 0.35671695\n",
      "Iteration 35, loss = 0.34831622\n",
      "Iteration 36, loss = 0.33529329\n",
      "Iteration 37, loss = 0.32594115\n",
      "Iteration 38, loss = 0.31938707\n",
      "Iteration 39, loss = 0.31576817\n",
      "Iteration 40, loss = 0.30595064\n",
      "Iteration 41, loss = 0.30279585\n",
      "Iteration 42, loss = 0.29506571\n",
      "Iteration 43, loss = 0.28691438\n",
      "Iteration 44, loss = 0.29358519\n",
      "Iteration 45, loss = 0.27694871\n",
      "Iteration 46, loss = 0.26819917\n",
      "Iteration 47, loss = 0.26343657\n",
      "Iteration 48, loss = 0.25770350\n",
      "Iteration 49, loss = 0.25571661\n",
      "Iteration 50, loss = 0.24484941\n",
      "Iteration 51, loss = 0.23947120\n",
      "Iteration 52, loss = 0.23884231\n",
      "Iteration 53, loss = 0.23218244\n",
      "Iteration 54, loss = 0.22707856\n",
      "Iteration 55, loss = 0.21944547\n",
      "Iteration 56, loss = 0.21844505\n",
      "Iteration 57, loss = 0.22601879\n",
      "Iteration 58, loss = 0.21287431\n",
      "Iteration 59, loss = 0.20750581\n",
      "Iteration 60, loss = 0.19411969\n",
      "Iteration 61, loss = 0.19640568\n",
      "Iteration 62, loss = 0.19265996\n",
      "Iteration 63, loss = 0.18589662\n",
      "Iteration 64, loss = 0.17889347\n",
      "Iteration 65, loss = 0.17751649\n",
      "Iteration 66, loss = 0.17675134\n",
      "Iteration 67, loss = 0.16793874\n",
      "Iteration 68, loss = 0.16371167\n",
      "Iteration 69, loss = 0.16084702\n",
      "Iteration 70, loss = 0.15433642\n",
      "Iteration 71, loss = 0.14877438\n",
      "Iteration 72, loss = 0.15407010\n",
      "Iteration 73, loss = 0.15648316\n",
      "Iteration 74, loss = 0.15085498\n",
      "Iteration 75, loss = 0.14055175\n",
      "Iteration 76, loss = 0.13638405\n",
      "Iteration 77, loss = 0.13161608\n",
      "Iteration 78, loss = 0.12811082\n",
      "Iteration 79, loss = 0.12878039\n",
      "Iteration 80, loss = 0.13732952\n",
      "Iteration 81, loss = 0.12623492\n",
      "Iteration 82, loss = 0.11887816\n",
      "Iteration 83, loss = 0.11640462\n",
      "Iteration 84, loss = 0.11413745\n",
      "Iteration 85, loss = 0.11972256\n",
      "Iteration 86, loss = 0.11431705\n",
      "Iteration 87, loss = 0.11624020\n",
      "Iteration 88, loss = 0.10590845\n",
      "Iteration 89, loss = 0.10554361\n",
      "Iteration 90, loss = 0.11416991\n",
      "Iteration 91, loss = 0.09870231\n",
      "Iteration 92, loss = 0.09048532\n",
      "Iteration 93, loss = 0.09416754\n",
      "Iteration 94, loss = 0.08915589\n",
      "Iteration 95, loss = 0.09083239\n",
      "Iteration 96, loss = 0.08733567\n",
      "Iteration 97, loss = 0.08432909\n",
      "Iteration 98, loss = 0.09262819\n",
      "Iteration 99, loss = 0.08794679\n",
      "Iteration 100, loss = 0.08213557\n",
      "Iteration 101, loss = 0.07843032\n",
      "Iteration 102, loss = 0.07480744\n",
      "Iteration 103, loss = 0.07464297\n",
      "Iteration 104, loss = 0.06959125\n",
      "Iteration 105, loss = 0.07029290\n",
      "Iteration 106, loss = 0.07111038\n",
      "Iteration 107, loss = 0.07333859\n",
      "Iteration 108, loss = 0.06565931\n",
      "Iteration 109, loss = 0.06193192\n",
      "Iteration 110, loss = 0.06176538\n",
      "Iteration 111, loss = 0.06307915\n",
      "Iteration 112, loss = 0.05888009\n",
      "Iteration 113, loss = 0.06408941\n",
      "Iteration 114, loss = 0.06036671\n",
      "Iteration 115, loss = 0.05536396\n",
      "Iteration 116, loss = 0.05231719\n",
      "Iteration 117, loss = 0.05441959\n",
      "Iteration 118, loss = 0.05384197\n",
      "Iteration 119, loss = 0.05292370\n",
      "Iteration 120, loss = 0.04867186\n",
      "Iteration 121, loss = 0.05128329\n",
      "Iteration 122, loss = 0.04848135\n",
      "Iteration 123, loss = 0.05282072\n",
      "Iteration 124, loss = 0.05108167\n",
      "Iteration 125, loss = 0.05569567\n",
      "Iteration 126, loss = 0.04576781\n",
      "Iteration 127, loss = 0.04734326\n",
      "Iteration 128, loss = 0.04280392\n",
      "Iteration 129, loss = 0.04203954\n",
      "Iteration 130, loss = 0.04176584\n",
      "Iteration 131, loss = 0.04252497\n",
      "Iteration 132, loss = 0.03973179\n",
      "Iteration 133, loss = 0.03679094\n",
      "Iteration 134, loss = 0.03528959\n",
      "Iteration 135, loss = 0.03609772\n",
      "Iteration 136, loss = 0.03569400\n",
      "Iteration 137, loss = 0.03333789\n",
      "Iteration 138, loss = 0.03304555\n",
      "Iteration 139, loss = 0.03464826\n",
      "Iteration 140, loss = 0.03202655\n",
      "Iteration 141, loss = 0.03343496\n",
      "Iteration 142, loss = 0.03207679\n",
      "Iteration 143, loss = 0.03330738\n",
      "Iteration 144, loss = 0.03232807\n",
      "Iteration 145, loss = 0.04309309\n",
      "Iteration 146, loss = 0.03876101\n",
      "Iteration 147, loss = 0.03950711\n",
      "Iteration 148, loss = 0.03347355\n",
      "Iteration 149, loss = 0.03763779\n",
      "Iteration 150, loss = 0.04271043\n",
      "Iteration 151, loss = 0.03954800\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66943988\n",
      "Iteration 2, loss = 1.23822304\n",
      "Iteration 3, loss = 1.05594042\n",
      "Iteration 4, loss = 0.91618624\n",
      "Iteration 5, loss = 0.82336653\n",
      "Iteration 6, loss = 0.75033152\n",
      "Iteration 7, loss = 0.70106646\n",
      "Iteration 8, loss = 0.66314366\n",
      "Iteration 9, loss = 0.63033590\n",
      "Iteration 10, loss = 0.60501792\n",
      "Iteration 11, loss = 0.58203235\n",
      "Iteration 12, loss = 0.56151470\n",
      "Iteration 13, loss = 0.54488383\n",
      "Iteration 14, loss = 0.52761137\n",
      "Iteration 15, loss = 0.51383368\n",
      "Iteration 16, loss = 0.50143890\n",
      "Iteration 17, loss = 0.48766714\n",
      "Iteration 18, loss = 0.47623509\n",
      "Iteration 19, loss = 0.46624334\n",
      "Iteration 20, loss = 0.46022292\n",
      "Iteration 21, loss = 0.45062033\n",
      "Iteration 22, loss = 0.44023219\n",
      "Iteration 23, loss = 0.43224024\n",
      "Iteration 24, loss = 0.42584589\n",
      "Iteration 25, loss = 0.41041575\n",
      "Iteration 26, loss = 0.40125733\n",
      "Iteration 27, loss = 0.39368265\n",
      "Iteration 28, loss = 0.38691027\n",
      "Iteration 29, loss = 0.38098518\n",
      "Iteration 30, loss = 0.37557109\n",
      "Iteration 31, loss = 0.36713088\n",
      "Iteration 32, loss = 0.36054060\n",
      "Iteration 33, loss = 0.35034279\n",
      "Iteration 34, loss = 0.34159212\n",
      "Iteration 35, loss = 0.34112779\n",
      "Iteration 36, loss = 0.33319686\n",
      "Iteration 37, loss = 0.32788628\n",
      "Iteration 38, loss = 0.31987642\n",
      "Iteration 39, loss = 0.31295649\n",
      "Iteration 40, loss = 0.30514362\n",
      "Iteration 41, loss = 0.30045355\n",
      "Iteration 42, loss = 0.30195344\n",
      "Iteration 43, loss = 0.28756567\n",
      "Iteration 44, loss = 0.28236711\n",
      "Iteration 45, loss = 0.28510184\n",
      "Iteration 46, loss = 0.28087290\n",
      "Iteration 47, loss = 0.27984484\n",
      "Iteration 48, loss = 0.26199785\n",
      "Iteration 49, loss = 0.25669763\n",
      "Iteration 50, loss = 0.25387685\n",
      "Iteration 51, loss = 0.26112339\n",
      "Iteration 52, loss = 0.25381483\n",
      "Iteration 53, loss = 0.24234467\n",
      "Iteration 54, loss = 0.23385798\n",
      "Iteration 55, loss = 0.23431932\n",
      "Iteration 56, loss = 0.22367320\n",
      "Iteration 57, loss = 0.21928073\n",
      "Iteration 58, loss = 0.22137007\n",
      "Iteration 59, loss = 0.22179298\n",
      "Iteration 60, loss = 0.20112927\n",
      "Iteration 61, loss = 0.19712869\n",
      "Iteration 62, loss = 0.19427922\n",
      "Iteration 63, loss = 0.19234891\n",
      "Iteration 64, loss = 0.18832245\n",
      "Iteration 65, loss = 0.18296809\n",
      "Iteration 66, loss = 0.18783778\n",
      "Iteration 67, loss = 0.18694195\n",
      "Iteration 68, loss = 0.18248348\n",
      "Iteration 69, loss = 0.17226686\n",
      "Iteration 70, loss = 0.16446856\n",
      "Iteration 71, loss = 0.16407308\n",
      "Iteration 72, loss = 0.15738490\n",
      "Iteration 73, loss = 0.17324260\n",
      "Iteration 74, loss = 0.16282119\n",
      "Iteration 75, loss = 0.15196353\n",
      "Iteration 76, loss = 0.14932841\n",
      "Iteration 77, loss = 0.14973139\n",
      "Iteration 78, loss = 0.13612904\n",
      "Iteration 79, loss = 0.13162478\n",
      "Iteration 80, loss = 0.13224054\n",
      "Iteration 81, loss = 0.13038333\n",
      "Iteration 82, loss = 0.13213064\n",
      "Iteration 83, loss = 0.12126566\n",
      "Iteration 84, loss = 0.11943382\n",
      "Iteration 85, loss = 0.11707860\n",
      "Iteration 86, loss = 0.11811926\n",
      "Iteration 87, loss = 0.11276145\n",
      "Iteration 88, loss = 0.11381057\n",
      "Iteration 89, loss = 0.10701406\n",
      "Iteration 90, loss = 0.10112203\n",
      "Iteration 91, loss = 0.10021229\n",
      "Iteration 92, loss = 0.10042342\n",
      "Iteration 93, loss = 0.10162944\n",
      "Iteration 94, loss = 0.10301628\n",
      "Iteration 95, loss = 0.11773799\n",
      "Iteration 96, loss = 0.11465076\n",
      "Iteration 97, loss = 0.10780722\n",
      "Iteration 98, loss = 0.10577435\n",
      "Iteration 99, loss = 0.09755399\n",
      "Iteration 100, loss = 0.09295573\n",
      "Iteration 101, loss = 0.08772693\n",
      "Iteration 102, loss = 0.08173578\n",
      "Iteration 103, loss = 0.07737373\n",
      "Iteration 104, loss = 0.08179165\n",
      "Iteration 105, loss = 0.07800256\n",
      "Iteration 106, loss = 0.07765563\n",
      "Iteration 107, loss = 0.07911057\n",
      "Iteration 108, loss = 0.07527313\n",
      "Iteration 109, loss = 0.07663162\n",
      "Iteration 110, loss = 0.08302615\n",
      "Iteration 111, loss = 0.06970556\n",
      "Iteration 112, loss = 0.06559174\n",
      "Iteration 113, loss = 0.06427716\n",
      "Iteration 114, loss = 0.06317391\n",
      "Iteration 115, loss = 0.05995782\n",
      "Iteration 116, loss = 0.05703882\n",
      "Iteration 117, loss = 0.05474510\n",
      "Iteration 118, loss = 0.05287509\n",
      "Iteration 119, loss = 0.05124872\n",
      "Iteration 120, loss = 0.05166026\n",
      "Iteration 121, loss = 0.05525050\n",
      "Iteration 122, loss = 0.05263247\n",
      "Iteration 123, loss = 0.05030363\n",
      "Iteration 124, loss = 0.05501801\n",
      "Iteration 125, loss = 0.04777412\n",
      "Iteration 126, loss = 0.04589476\n",
      "Iteration 127, loss = 0.04528846\n",
      "Iteration 128, loss = 0.04817678\n",
      "Iteration 129, loss = 0.05315265\n",
      "Iteration 130, loss = 0.07008599\n",
      "Iteration 131, loss = 0.05623385\n",
      "Iteration 132, loss = 0.05184488\n",
      "Iteration 133, loss = 0.05034651\n",
      "Iteration 134, loss = 0.04807445\n",
      "Iteration 135, loss = 0.04191340\n",
      "Iteration 136, loss = 0.03976188\n",
      "Iteration 137, loss = 0.03934868\n",
      "Iteration 138, loss = 0.03468915\n",
      "Iteration 139, loss = 0.03253731\n",
      "Iteration 140, loss = 0.03263272\n",
      "Iteration 141, loss = 0.03281796\n",
      "Iteration 142, loss = 0.03777641\n",
      "Iteration 143, loss = 0.03478087\n",
      "Iteration 144, loss = 0.03628819\n",
      "Iteration 145, loss = 0.04313879\n",
      "Iteration 146, loss = 0.03520720\n",
      "Iteration 147, loss = 0.03473262\n",
      "Iteration 148, loss = 0.03077530\n",
      "Iteration 149, loss = 0.02782089\n",
      "Iteration 150, loss = 0.02737056\n",
      "Iteration 151, loss = 0.03211884\n",
      "Iteration 152, loss = 0.02841688\n",
      "Iteration 153, loss = 0.02595452\n",
      "Iteration 154, loss = 0.02662574\n",
      "Iteration 155, loss = 0.02441902\n",
      "Iteration 156, loss = 0.02684962\n",
      "Iteration 157, loss = 0.02385139\n",
      "Iteration 158, loss = 0.02417007\n",
      "Iteration 159, loss = 0.02202483\n",
      "Iteration 160, loss = 0.02234049\n",
      "Iteration 161, loss = 0.02437981\n",
      "Iteration 162, loss = 0.02404993\n",
      "Iteration 163, loss = 0.02346653\n",
      "Iteration 164, loss = 0.02269046\n",
      "Iteration 165, loss = 0.02278392\n",
      "Iteration 166, loss = 0.02032740\n",
      "Iteration 167, loss = 0.01979265\n",
      "Iteration 168, loss = 0.01967426\n",
      "Iteration 169, loss = 0.01845531\n",
      "Iteration 170, loss = 0.01755722\n",
      "Iteration 171, loss = 0.01771171\n",
      "Iteration 172, loss = 0.01877331\n",
      "Iteration 173, loss = 0.02165207\n",
      "Iteration 174, loss = 0.02176173\n",
      "Iteration 175, loss = 0.01938059\n",
      "Iteration 176, loss = 0.01711630\n",
      "Iteration 177, loss = 0.01615492\n",
      "Iteration 178, loss = 0.01824858\n",
      "Iteration 179, loss = 0.01624454\n",
      "Iteration 180, loss = 0.01625132\n",
      "Iteration 181, loss = 0.01748985\n",
      "Iteration 182, loss = 0.01908265\n",
      "Iteration 183, loss = 0.01964143\n",
      "Iteration 184, loss = 0.02023855\n",
      "Iteration 185, loss = 0.01860301\n",
      "Iteration 186, loss = 0.01737965\n",
      "Iteration 187, loss = 0.01592651\n",
      "Iteration 188, loss = 0.01591698\n",
      "Iteration 189, loss = 0.01547379\n",
      "Iteration 190, loss = 0.01758037\n",
      "Iteration 191, loss = 0.02250420\n",
      "Iteration 192, loss = 0.02231580\n",
      "Iteration 193, loss = 0.01657743\n",
      "Iteration 194, loss = 0.02113865\n",
      "Iteration 195, loss = 0.02131064\n",
      "Iteration 196, loss = 0.02680844\n",
      "Iteration 197, loss = 0.01955189\n",
      "Iteration 198, loss = 0.01626285\n",
      "Iteration 199, loss = 0.01481347\n",
      "Iteration 200, loss = 0.01384241\n",
      "Iteration 201, loss = 0.01174916\n",
      "Iteration 202, loss = 0.01186582\n",
      "Iteration 203, loss = 0.01109386\n",
      "Iteration 204, loss = 0.01129986\n",
      "Iteration 205, loss = 0.01240513\n",
      "Iteration 206, loss = 0.01171114\n",
      "Iteration 207, loss = 0.01088911\n",
      "Iteration 208, loss = 0.00998647\n",
      "Iteration 209, loss = 0.01090328\n",
      "Iteration 210, loss = 0.01005405\n",
      "Iteration 211, loss = 0.01020945\n",
      "Iteration 212, loss = 0.00962402\n",
      "Iteration 213, loss = 0.01180073\n",
      "Iteration 214, loss = 0.01043637\n",
      "Iteration 215, loss = 0.00963172\n",
      "Iteration 216, loss = 0.00934560\n",
      "Iteration 217, loss = 0.00958956\n",
      "Iteration 218, loss = 0.00964567\n",
      "Iteration 219, loss = 0.01344050\n",
      "Iteration 220, loss = 0.02124736\n",
      "Iteration 221, loss = 0.04455519\n",
      "Iteration 222, loss = 0.02564516\n",
      "Iteration 223, loss = 0.02284268\n",
      "Iteration 224, loss = 0.01680799\n",
      "Iteration 225, loss = 0.01495375\n",
      "Iteration 226, loss = 0.01848332\n",
      "Iteration 227, loss = 0.02938768\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.73518099\n",
      "Iteration 2, loss = 1.31113024\n",
      "Iteration 3, loss = 1.10175120\n",
      "Iteration 4, loss = 0.96427042\n",
      "Iteration 5, loss = 0.86164932\n",
      "Iteration 6, loss = 0.78433812\n",
      "Iteration 7, loss = 0.72378806\n",
      "Iteration 8, loss = 0.67969732\n",
      "Iteration 9, loss = 0.64711132\n",
      "Iteration 10, loss = 0.62073615\n",
      "Iteration 11, loss = 0.59421322\n",
      "Iteration 12, loss = 0.57172277\n",
      "Iteration 13, loss = 0.55488657\n",
      "Iteration 14, loss = 0.54379726\n",
      "Iteration 15, loss = 0.52804614\n",
      "Iteration 16, loss = 0.51653392\n",
      "Iteration 17, loss = 0.50554117\n",
      "Iteration 18, loss = 0.49093451\n",
      "Iteration 19, loss = 0.48357137\n",
      "Iteration 20, loss = 0.47115708\n",
      "Iteration 21, loss = 0.46269990\n",
      "Iteration 22, loss = 0.46108047\n",
      "Iteration 23, loss = 0.45828359\n",
      "Iteration 24, loss = 0.43771297\n",
      "Iteration 25, loss = 0.42961852\n",
      "Iteration 26, loss = 0.42625908\n",
      "Iteration 27, loss = 0.41264687\n",
      "Iteration 28, loss = 0.40349388\n",
      "Iteration 29, loss = 0.39578441\n",
      "Iteration 30, loss = 0.38831367\n",
      "Iteration 31, loss = 0.38207416\n",
      "Iteration 32, loss = 0.37174839\n",
      "Iteration 33, loss = 0.36481454\n",
      "Iteration 34, loss = 0.36024521\n",
      "Iteration 35, loss = 0.35569106\n",
      "Iteration 36, loss = 0.35390648\n",
      "Iteration 37, loss = 0.34386671\n",
      "Iteration 38, loss = 0.33675371\n",
      "Iteration 39, loss = 0.33403971\n",
      "Iteration 40, loss = 0.33163345\n",
      "Iteration 41, loss = 0.32085586\n",
      "Iteration 42, loss = 0.31415232\n",
      "Iteration 43, loss = 0.30230857\n",
      "Iteration 44, loss = 0.30199228\n",
      "Iteration 45, loss = 0.29546146\n",
      "Iteration 46, loss = 0.28668521\n",
      "Iteration 47, loss = 0.28167055\n",
      "Iteration 48, loss = 0.27847831\n",
      "Iteration 49, loss = 0.27760410\n",
      "Iteration 50, loss = 0.26623183\n",
      "Iteration 51, loss = 0.25866318\n",
      "Iteration 52, loss = 0.25890840\n",
      "Iteration 53, loss = 0.25818191\n",
      "Iteration 54, loss = 0.26115887\n",
      "Iteration 55, loss = 0.24376649\n",
      "Iteration 56, loss = 0.23371045\n",
      "Iteration 57, loss = 0.23308392\n",
      "Iteration 58, loss = 0.23056661\n",
      "Iteration 59, loss = 0.22323220\n",
      "Iteration 60, loss = 0.20934638\n",
      "Iteration 61, loss = 0.20677205\n",
      "Iteration 62, loss = 0.20144234\n",
      "Iteration 63, loss = 0.19780266\n",
      "Iteration 64, loss = 0.19594503\n",
      "Iteration 65, loss = 0.19079949\n",
      "Iteration 66, loss = 0.20023066\n",
      "Iteration 67, loss = 0.18830626\n",
      "Iteration 68, loss = 0.17594167\n",
      "Iteration 69, loss = 0.17743820\n",
      "Iteration 70, loss = 0.16420753\n",
      "Iteration 71, loss = 0.16587227\n",
      "Iteration 72, loss = 0.15873975\n",
      "Iteration 73, loss = 0.15833275\n",
      "Iteration 74, loss = 0.15881034\n",
      "Iteration 75, loss = 0.15318505\n",
      "Iteration 76, loss = 0.14733165\n",
      "Iteration 77, loss = 0.14699154\n",
      "Iteration 78, loss = 0.16054063\n",
      "Iteration 79, loss = 0.15149746\n",
      "Iteration 80, loss = 0.14657819\n",
      "Iteration 81, loss = 0.14651758\n",
      "Iteration 82, loss = 0.13757956\n",
      "Iteration 83, loss = 0.13712288\n",
      "Iteration 84, loss = 0.13292951\n",
      "Iteration 85, loss = 0.12589474\n",
      "Iteration 86, loss = 0.12063595\n",
      "Iteration 87, loss = 0.11215687\n",
      "Iteration 88, loss = 0.11725462\n",
      "Iteration 89, loss = 0.11717433\n",
      "Iteration 90, loss = 0.10932752\n",
      "Iteration 91, loss = 0.11029420\n",
      "Iteration 92, loss = 0.10478073\n",
      "Iteration 93, loss = 0.09961731\n",
      "Iteration 94, loss = 0.09970042\n",
      "Iteration 95, loss = 0.09270146\n",
      "Iteration 96, loss = 0.10053813\n",
      "Iteration 97, loss = 0.10020027\n",
      "Iteration 98, loss = 0.09185466\n",
      "Iteration 99, loss = 0.09471756\n",
      "Iteration 100, loss = 0.09501232\n",
      "Iteration 101, loss = 0.09547395\n",
      "Iteration 102, loss = 0.08653110\n",
      "Iteration 103, loss = 0.08471039\n",
      "Iteration 104, loss = 0.08561776\n",
      "Iteration 105, loss = 0.07616176\n",
      "Iteration 106, loss = 0.07342589\n",
      "Iteration 107, loss = 0.07123985\n",
      "Iteration 108, loss = 0.06935068\n",
      "Iteration 109, loss = 0.06990410\n",
      "Iteration 110, loss = 0.06949947\n",
      "Iteration 111, loss = 0.06377051\n",
      "Iteration 112, loss = 0.06706148\n",
      "Iteration 113, loss = 0.06522970\n",
      "Iteration 114, loss = 0.06733360\n",
      "Iteration 115, loss = 0.05900189\n",
      "Iteration 116, loss = 0.06352881\n",
      "Iteration 117, loss = 0.05714176\n",
      "Iteration 118, loss = 0.05967799\n",
      "Iteration 119, loss = 0.05720614\n",
      "Iteration 120, loss = 0.05718217\n",
      "Iteration 121, loss = 0.04912661\n",
      "Iteration 122, loss = 0.05507697\n",
      "Iteration 123, loss = 0.05239849\n",
      "Iteration 124, loss = 0.04925475\n",
      "Iteration 125, loss = 0.04995103\n",
      "Iteration 126, loss = 0.05157524\n",
      "Iteration 127, loss = 0.05365556\n",
      "Iteration 128, loss = 0.04988010\n",
      "Iteration 129, loss = 0.04796959\n",
      "Iteration 130, loss = 0.05701045\n",
      "Iteration 131, loss = 0.06408619\n",
      "Iteration 132, loss = 0.04905389\n",
      "Iteration 133, loss = 0.04563606\n",
      "Iteration 134, loss = 0.04549510\n",
      "Iteration 135, loss = 0.04325007\n",
      "Iteration 136, loss = 0.04019441\n",
      "Iteration 137, loss = 0.03967326\n",
      "Iteration 138, loss = 0.03698456\n",
      "Iteration 139, loss = 0.03870999\n",
      "Iteration 140, loss = 0.03514915\n",
      "Iteration 141, loss = 0.03603161\n",
      "Iteration 142, loss = 0.03398579\n",
      "Iteration 143, loss = 0.03544371\n",
      "Iteration 144, loss = 0.03561385\n",
      "Iteration 145, loss = 0.03470881\n",
      "Iteration 146, loss = 0.03488934\n",
      "Iteration 147, loss = 0.03657960\n",
      "Iteration 148, loss = 0.03465864\n",
      "Iteration 149, loss = 0.03193067\n",
      "Iteration 150, loss = 0.03490615\n",
      "Iteration 151, loss = 0.03778520\n",
      "Iteration 152, loss = 0.03798540\n",
      "Iteration 153, loss = 0.03172358\n",
      "Iteration 154, loss = 0.02765874\n",
      "Iteration 155, loss = 0.02698676\n",
      "Iteration 156, loss = 0.02745352\n",
      "Iteration 157, loss = 0.02579764\n",
      "Iteration 158, loss = 0.02527481\n",
      "Iteration 159, loss = 0.02548698\n",
      "Iteration 160, loss = 0.02502732\n",
      "Iteration 161, loss = 0.02285400\n",
      "Iteration 162, loss = 0.02338224\n",
      "Iteration 163, loss = 0.02545469\n",
      "Iteration 164, loss = 0.02633252\n",
      "Iteration 165, loss = 0.02843505\n",
      "Iteration 166, loss = 0.02521661\n",
      "Iteration 167, loss = 0.02461952\n",
      "Iteration 168, loss = 0.02589632\n",
      "Iteration 169, loss = 0.02349935\n",
      "Iteration 170, loss = 0.02339652\n",
      "Iteration 171, loss = 0.02220013\n",
      "Iteration 172, loss = 0.01947713\n",
      "Iteration 173, loss = 0.02144390\n",
      "Iteration 174, loss = 0.02301984\n",
      "Iteration 175, loss = 0.02277644\n",
      "Iteration 176, loss = 0.02315770\n",
      "Iteration 177, loss = 0.01907753\n",
      "Iteration 178, loss = 0.01845397\n",
      "Iteration 179, loss = 0.01745200\n",
      "Iteration 180, loss = 0.01851290\n",
      "Iteration 181, loss = 0.01999902\n",
      "Iteration 182, loss = 0.02021447\n",
      "Iteration 183, loss = 0.01832981\n",
      "Iteration 184, loss = 0.01677598\n",
      "Iteration 185, loss = 0.01703414\n",
      "Iteration 186, loss = 0.02014432\n",
      "Iteration 187, loss = 0.02123888\n",
      "Iteration 188, loss = 0.01923212\n",
      "Iteration 189, loss = 0.02054497\n",
      "Iteration 190, loss = 0.01748443\n",
      "Iteration 191, loss = 0.01536995\n",
      "Iteration 192, loss = 0.01490976\n",
      "Iteration 193, loss = 0.01630288\n",
      "Iteration 194, loss = 0.01681827\n",
      "Iteration 195, loss = 0.01612057\n",
      "Iteration 196, loss = 0.02452558\n",
      "Iteration 197, loss = 0.01526378\n",
      "Iteration 198, loss = 0.01595531\n",
      "Iteration 199, loss = 0.01388995\n",
      "Iteration 200, loss = 0.01448474\n",
      "Iteration 201, loss = 0.01489649\n",
      "Iteration 202, loss = 0.02051353\n",
      "Iteration 203, loss = 0.02085932\n",
      "Iteration 204, loss = 0.01875412\n",
      "Iteration 205, loss = 0.02890125\n",
      "Iteration 206, loss = 0.01903891\n",
      "Iteration 207, loss = 0.01537800\n",
      "Iteration 208, loss = 0.01532911\n",
      "Iteration 209, loss = 0.01406874\n",
      "Iteration 210, loss = 0.01287385\n",
      "Iteration 211, loss = 0.01226736\n",
      "Iteration 212, loss = 0.01258461\n",
      "Iteration 213, loss = 0.01143654\n",
      "Iteration 214, loss = 0.01299610\n",
      "Iteration 215, loss = 0.01136002\n",
      "Iteration 216, loss = 0.01172140\n",
      "Iteration 217, loss = 0.01085836\n",
      "Iteration 218, loss = 0.01023036\n",
      "Iteration 219, loss = 0.01131560\n",
      "Iteration 220, loss = 0.01192339\n",
      "Iteration 221, loss = 0.01199177\n",
      "Iteration 222, loss = 0.01259049\n",
      "Iteration 223, loss = 0.01406041\n",
      "Iteration 224, loss = 0.01593115\n",
      "Iteration 225, loss = 0.01184791\n",
      "Iteration 226, loss = 0.01085580\n",
      "Iteration 227, loss = 0.01227223\n",
      "Iteration 228, loss = 0.01113456\n",
      "Iteration 229, loss = 0.01065496\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.76322474\n",
      "Iteration 2, loss = 1.31628570\n",
      "Iteration 3, loss = 1.11497069\n",
      "Iteration 4, loss = 0.96729121\n",
      "Iteration 5, loss = 0.85118891\n",
      "Iteration 6, loss = 0.76952888\n",
      "Iteration 7, loss = 0.71493146\n",
      "Iteration 8, loss = 0.67110069\n",
      "Iteration 9, loss = 0.63919333\n",
      "Iteration 10, loss = 0.61345440\n",
      "Iteration 11, loss = 0.59662289\n",
      "Iteration 12, loss = 0.57496348\n",
      "Iteration 13, loss = 0.55702610\n",
      "Iteration 14, loss = 0.53813778\n",
      "Iteration 15, loss = 0.52156008\n",
      "Iteration 16, loss = 0.51107507\n",
      "Iteration 17, loss = 0.49617082\n",
      "Iteration 18, loss = 0.48819445\n",
      "Iteration 19, loss = 0.47697711\n",
      "Iteration 20, loss = 0.46584795\n",
      "Iteration 21, loss = 0.45644546\n",
      "Iteration 22, loss = 0.44412738\n",
      "Iteration 23, loss = 0.43052895\n",
      "Iteration 24, loss = 0.42445768\n",
      "Iteration 25, loss = 0.41476520\n",
      "Iteration 26, loss = 0.40608055\n",
      "Iteration 27, loss = 0.39273838\n",
      "Iteration 28, loss = 0.38646428\n",
      "Iteration 29, loss = 0.38067820\n",
      "Iteration 30, loss = 0.37344186\n",
      "Iteration 31, loss = 0.37111922\n",
      "Iteration 32, loss = 0.36813227\n",
      "Iteration 33, loss = 0.35182149\n",
      "Iteration 34, loss = 0.33661898\n",
      "Iteration 35, loss = 0.33013922\n",
      "Iteration 36, loss = 0.32362432\n",
      "Iteration 37, loss = 0.31713540\n",
      "Iteration 38, loss = 0.31789681\n",
      "Iteration 39, loss = 0.31285398\n",
      "Iteration 40, loss = 0.30549477\n",
      "Iteration 41, loss = 0.29199099\n",
      "Iteration 42, loss = 0.29101577\n",
      "Iteration 43, loss = 0.28349940\n",
      "Iteration 44, loss = 0.27166850\n",
      "Iteration 45, loss = 0.26414464\n",
      "Iteration 46, loss = 0.25796475\n",
      "Iteration 47, loss = 0.25217783\n",
      "Iteration 48, loss = 0.24293137\n",
      "Iteration 49, loss = 0.23743831\n",
      "Iteration 50, loss = 0.23357029\n",
      "Iteration 51, loss = 0.22390833\n",
      "Iteration 52, loss = 0.22398078\n",
      "Iteration 53, loss = 0.21655481\n",
      "Iteration 54, loss = 0.21569387\n",
      "Iteration 55, loss = 0.21224005\n",
      "Iteration 56, loss = 0.20656585\n",
      "Iteration 57, loss = 0.19833599\n",
      "Iteration 58, loss = 0.19881473\n",
      "Iteration 59, loss = 0.18891388\n",
      "Iteration 60, loss = 0.18603732\n",
      "Iteration 61, loss = 0.17518832\n",
      "Iteration 62, loss = 0.16887680\n",
      "Iteration 63, loss = 0.16731289\n",
      "Iteration 64, loss = 0.16578666\n",
      "Iteration 65, loss = 0.16331985\n",
      "Iteration 66, loss = 0.15269973\n",
      "Iteration 67, loss = 0.15064405\n",
      "Iteration 68, loss = 0.15326986\n",
      "Iteration 69, loss = 0.16045226\n",
      "Iteration 70, loss = 0.15221321\n",
      "Iteration 71, loss = 0.14172431\n",
      "Iteration 72, loss = 0.13545000\n",
      "Iteration 73, loss = 0.13092558\n",
      "Iteration 74, loss = 0.12528533\n",
      "Iteration 75, loss = 0.12105585\n",
      "Iteration 76, loss = 0.12653073\n",
      "Iteration 77, loss = 0.11947162\n",
      "Iteration 78, loss = 0.11864606\n",
      "Iteration 79, loss = 0.11064834\n",
      "Iteration 80, loss = 0.11722069\n",
      "Iteration 81, loss = 0.11076441\n",
      "Iteration 82, loss = 0.10822994\n",
      "Iteration 83, loss = 0.11097878\n",
      "Iteration 84, loss = 0.10040619\n",
      "Iteration 85, loss = 0.09799620\n",
      "Iteration 86, loss = 0.09189719\n",
      "Iteration 87, loss = 0.09314838\n",
      "Iteration 88, loss = 0.09541585\n",
      "Iteration 89, loss = 0.08731492\n",
      "Iteration 90, loss = 0.08487142\n",
      "Iteration 91, loss = 0.08091073\n",
      "Iteration 92, loss = 0.07731756\n",
      "Iteration 93, loss = 0.08096347\n",
      "Iteration 94, loss = 0.07720620\n",
      "Iteration 95, loss = 0.07656995\n",
      "Iteration 96, loss = 0.07437845\n",
      "Iteration 97, loss = 0.07116037\n",
      "Iteration 98, loss = 0.07287910\n",
      "Iteration 99, loss = 0.06946604\n",
      "Iteration 100, loss = 0.06673017\n",
      "Iteration 101, loss = 0.06358640\n",
      "Iteration 102, loss = 0.06154799\n",
      "Iteration 103, loss = 0.06650034\n",
      "Iteration 104, loss = 0.06737856\n",
      "Iteration 105, loss = 0.06693472\n",
      "Iteration 106, loss = 0.06359506\n",
      "Iteration 107, loss = 0.06416895\n",
      "Iteration 108, loss = 0.06338156\n",
      "Iteration 109, loss = 0.05615543\n",
      "Iteration 110, loss = 0.05188590\n",
      "Iteration 111, loss = 0.04885755\n",
      "Iteration 112, loss = 0.04762430\n",
      "Iteration 113, loss = 0.04914895\n",
      "Iteration 114, loss = 0.04564908\n",
      "Iteration 115, loss = 0.04474551\n",
      "Iteration 116, loss = 0.04332599\n",
      "Iteration 117, loss = 0.04482092\n",
      "Iteration 118, loss = 0.04579582\n",
      "Iteration 119, loss = 0.04762559\n",
      "Iteration 120, loss = 0.05522989\n",
      "Iteration 121, loss = 0.05013758\n",
      "Iteration 122, loss = 0.04352820\n",
      "Iteration 123, loss = 0.04677579\n",
      "Iteration 124, loss = 0.04321135\n",
      "Iteration 125, loss = 0.04459253\n",
      "Iteration 126, loss = 0.04119544\n",
      "Iteration 127, loss = 0.03835065\n",
      "Iteration 128, loss = 0.03574459\n",
      "Iteration 129, loss = 0.03553502\n",
      "Iteration 130, loss = 0.03728042\n",
      "Iteration 131, loss = 0.03392135\n",
      "Iteration 132, loss = 0.03299925\n",
      "Iteration 133, loss = 0.02951720\n",
      "Iteration 134, loss = 0.02938999\n",
      "Iteration 135, loss = 0.03130191\n",
      "Iteration 136, loss = 0.03357308\n",
      "Iteration 137, loss = 0.03009152\n",
      "Iteration 138, loss = 0.02969195\n",
      "Iteration 139, loss = 0.02904441\n",
      "Iteration 140, loss = 0.02725303\n",
      "Iteration 141, loss = 0.02783198\n",
      "Iteration 142, loss = 0.02943572\n",
      "Iteration 143, loss = 0.02816167\n",
      "Iteration 144, loss = 0.02687419\n",
      "Iteration 145, loss = 0.02408006\n",
      "Iteration 146, loss = 0.02466489\n",
      "Iteration 147, loss = 0.02561678\n",
      "Iteration 148, loss = 0.02310925\n",
      "Iteration 149, loss = 0.02171940\n",
      "Iteration 150, loss = 0.02180202\n",
      "Iteration 151, loss = 0.02081294\n",
      "Iteration 152, loss = 0.02255191\n",
      "Iteration 153, loss = 0.02326468\n",
      "Iteration 154, loss = 0.02072385\n",
      "Iteration 155, loss = 0.01966564\n",
      "Iteration 156, loss = 0.02055087\n",
      "Iteration 157, loss = 0.02237143\n",
      "Iteration 158, loss = 0.02343795\n",
      "Iteration 159, loss = 0.02236326\n",
      "Iteration 160, loss = 0.02008321\n",
      "Iteration 161, loss = 0.02010346\n",
      "Iteration 162, loss = 0.02076444\n",
      "Iteration 163, loss = 0.01987198\n",
      "Iteration 164, loss = 0.01964023\n",
      "Iteration 165, loss = 0.01774756\n",
      "Iteration 166, loss = 0.01648985\n",
      "Iteration 167, loss = 0.01656297\n",
      "Iteration 168, loss = 0.01534821\n",
      "Iteration 169, loss = 0.01442099\n",
      "Iteration 170, loss = 0.01885725\n",
      "Iteration 171, loss = 0.01678956\n",
      "Iteration 172, loss = 0.01715068\n",
      "Iteration 173, loss = 0.01965898\n",
      "Iteration 174, loss = 0.01912861\n",
      "Iteration 175, loss = 0.02261086\n",
      "Iteration 176, loss = 0.02209844\n",
      "Iteration 177, loss = 0.01982588\n",
      "Iteration 178, loss = 0.02000754\n",
      "Iteration 179, loss = 0.02009168\n",
      "Iteration 180, loss = 0.01900674\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.72673707\n",
      "Iteration 2, loss = 1.31083588\n",
      "Iteration 3, loss = 1.07794890\n",
      "Iteration 4, loss = 0.92067439\n",
      "Iteration 5, loss = 0.81939962\n",
      "Iteration 6, loss = 0.75210308\n",
      "Iteration 7, loss = 0.70653582\n",
      "Iteration 8, loss = 0.66951777\n",
      "Iteration 9, loss = 0.63859128\n",
      "Iteration 10, loss = 0.61793497\n",
      "Iteration 11, loss = 0.59010986\n",
      "Iteration 12, loss = 0.56786539\n",
      "Iteration 13, loss = 0.55171166\n",
      "Iteration 14, loss = 0.53761600\n",
      "Iteration 15, loss = 0.52525400\n",
      "Iteration 16, loss = 0.51089792\n",
      "Iteration 17, loss = 0.49528437\n",
      "Iteration 18, loss = 0.48250936\n",
      "Iteration 19, loss = 0.47002304\n",
      "Iteration 20, loss = 0.46385213\n",
      "Iteration 21, loss = 0.45420283\n",
      "Iteration 22, loss = 0.44826219\n",
      "Iteration 23, loss = 0.43209906\n",
      "Iteration 24, loss = 0.43135745\n",
      "Iteration 25, loss = 0.42606861\n",
      "Iteration 26, loss = 0.40929490\n",
      "Iteration 27, loss = 0.40233806\n",
      "Iteration 28, loss = 0.39089835\n",
      "Iteration 29, loss = 0.38285102\n",
      "Iteration 30, loss = 0.37751868\n",
      "Iteration 31, loss = 0.37067127\n",
      "Iteration 32, loss = 0.36453969\n",
      "Iteration 33, loss = 0.35782343\n",
      "Iteration 34, loss = 0.34679656\n",
      "Iteration 35, loss = 0.33996683\n",
      "Iteration 36, loss = 0.33535139\n",
      "Iteration 37, loss = 0.32801370\n",
      "Iteration 38, loss = 0.32725633\n",
      "Iteration 39, loss = 0.31964444\n",
      "Iteration 40, loss = 0.31185228\n",
      "Iteration 41, loss = 0.30258261\n",
      "Iteration 42, loss = 0.30273064\n",
      "Iteration 43, loss = 0.29661128\n",
      "Iteration 44, loss = 0.28701432\n",
      "Iteration 45, loss = 0.28815743\n",
      "Iteration 46, loss = 0.28599313\n",
      "Iteration 47, loss = 0.27044820\n",
      "Iteration 48, loss = 0.26083823\n",
      "Iteration 49, loss = 0.25756462\n",
      "Iteration 50, loss = 0.26130189\n",
      "Iteration 51, loss = 0.24880268\n",
      "Iteration 52, loss = 0.23903973\n",
      "Iteration 53, loss = 0.23595481\n",
      "Iteration 54, loss = 0.23426130\n",
      "Iteration 55, loss = 0.22310520\n",
      "Iteration 56, loss = 0.21791216\n",
      "Iteration 57, loss = 0.21240158\n",
      "Iteration 58, loss = 0.20890029\n",
      "Iteration 59, loss = 0.20453023\n",
      "Iteration 60, loss = 0.19299234\n",
      "Iteration 61, loss = 0.19826877\n",
      "Iteration 62, loss = 0.18763751\n",
      "Iteration 63, loss = 0.18395420\n",
      "Iteration 64, loss = 0.18589930\n",
      "Iteration 65, loss = 0.18118474\n",
      "Iteration 66, loss = 0.17392925\n",
      "Iteration 67, loss = 0.16766224\n",
      "Iteration 68, loss = 0.17053538\n",
      "Iteration 69, loss = 0.18334384\n",
      "Iteration 70, loss = 0.18300758\n",
      "Iteration 71, loss = 0.16513044\n",
      "Iteration 72, loss = 0.15595547\n",
      "Iteration 73, loss = 0.15566595\n",
      "Iteration 74, loss = 0.14818346\n",
      "Iteration 75, loss = 0.14421330\n",
      "Iteration 76, loss = 0.13899542\n",
      "Iteration 77, loss = 0.13605906\n",
      "Iteration 78, loss = 0.13433867\n",
      "Iteration 79, loss = 0.12965316\n",
      "Iteration 80, loss = 0.13386932\n",
      "Iteration 81, loss = 0.14349308\n",
      "Iteration 82, loss = 0.12596551\n",
      "Iteration 83, loss = 0.11909343\n",
      "Iteration 84, loss = 0.11676293\n",
      "Iteration 85, loss = 0.11132908\n",
      "Iteration 86, loss = 0.10745448\n",
      "Iteration 87, loss = 0.10479273\n",
      "Iteration 88, loss = 0.10623874\n",
      "Iteration 89, loss = 0.10058880\n",
      "Iteration 90, loss = 0.10106397\n",
      "Iteration 91, loss = 0.09753347\n",
      "Iteration 92, loss = 0.09926261\n",
      "Iteration 93, loss = 0.09197656\n",
      "Iteration 94, loss = 0.09433197\n",
      "Iteration 95, loss = 0.10033683\n",
      "Iteration 96, loss = 0.09804842\n",
      "Iteration 97, loss = 0.08982449\n",
      "Iteration 98, loss = 0.08589595\n",
      "Iteration 99, loss = 0.08273143\n",
      "Iteration 100, loss = 0.07927097\n",
      "Iteration 101, loss = 0.07826395\n",
      "Iteration 102, loss = 0.08145348\n",
      "Iteration 103, loss = 0.07449104\n",
      "Iteration 104, loss = 0.07344915\n",
      "Iteration 105, loss = 0.07353973\n",
      "Iteration 106, loss = 0.07366981\n",
      "Iteration 107, loss = 0.06999178\n",
      "Iteration 108, loss = 0.06850237\n",
      "Iteration 109, loss = 0.06560115\n",
      "Iteration 110, loss = 0.06200236\n",
      "Iteration 111, loss = 0.06093301\n",
      "Iteration 112, loss = 0.06668907\n",
      "Iteration 113, loss = 0.06715097\n",
      "Iteration 114, loss = 0.06519349\n",
      "Iteration 115, loss = 0.06269382\n",
      "Iteration 116, loss = 0.06710779\n",
      "Iteration 117, loss = 0.05696613\n",
      "Iteration 118, loss = 0.05569001\n",
      "Iteration 119, loss = 0.05570692\n",
      "Iteration 120, loss = 0.05286283\n",
      "Iteration 121, loss = 0.05815536\n",
      "Iteration 122, loss = 0.05617669\n",
      "Iteration 123, loss = 0.05387290\n",
      "Iteration 124, loss = 0.04891944\n",
      "Iteration 125, loss = 0.04842350\n",
      "Iteration 126, loss = 0.04777380\n",
      "Iteration 127, loss = 0.04203392\n",
      "Iteration 128, loss = 0.04614247\n",
      "Iteration 129, loss = 0.04699727\n",
      "Iteration 130, loss = 0.05112471\n",
      "Iteration 131, loss = 0.04577551\n",
      "Iteration 132, loss = 0.04772762\n",
      "Iteration 133, loss = 0.04731435\n",
      "Iteration 134, loss = 0.04279614\n",
      "Iteration 135, loss = 0.04090118\n",
      "Iteration 136, loss = 0.04225891\n",
      "Iteration 137, loss = 0.04543414\n",
      "Iteration 138, loss = 0.05104249\n",
      "Iteration 139, loss = 0.05454916\n",
      "Iteration 140, loss = 0.04707766\n",
      "Iteration 141, loss = 0.03970256\n",
      "Iteration 142, loss = 0.03941313\n",
      "Iteration 143, loss = 0.03565405\n",
      "Iteration 144, loss = 0.03440337\n",
      "Iteration 145, loss = 0.03363600\n",
      "Iteration 146, loss = 0.03472646\n",
      "Iteration 147, loss = 0.03295853\n",
      "Iteration 148, loss = 0.03266640\n",
      "Iteration 149, loss = 0.03312765\n",
      "Iteration 150, loss = 0.03161325\n",
      "Iteration 151, loss = 0.03277697\n",
      "Iteration 152, loss = 0.03051576\n",
      "Iteration 153, loss = 0.02801741\n",
      "Iteration 154, loss = 0.02489326\n",
      "Iteration 155, loss = 0.02514862\n",
      "Iteration 156, loss = 0.03051864\n",
      "Iteration 157, loss = 0.02841068\n",
      "Iteration 158, loss = 0.02809020\n",
      "Iteration 159, loss = 0.02597771\n",
      "Iteration 160, loss = 0.02988576\n",
      "Iteration 161, loss = 0.02899647\n",
      "Iteration 162, loss = 0.02610136\n",
      "Iteration 163, loss = 0.02721146\n",
      "Iteration 164, loss = 0.02727188\n",
      "Iteration 165, loss = 0.04002945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.80199638\n",
      "Iteration 2, loss = 1.36271775\n",
      "Iteration 3, loss = 1.12639286\n",
      "Iteration 4, loss = 0.98340857\n",
      "Iteration 5, loss = 0.87004949\n",
      "Iteration 6, loss = 0.79244698\n",
      "Iteration 7, loss = 0.73312718\n",
      "Iteration 8, loss = 0.69346302\n",
      "Iteration 9, loss = 0.65205405\n",
      "Iteration 10, loss = 0.62828924\n",
      "Iteration 11, loss = 0.60324504\n",
      "Iteration 12, loss = 0.57659591\n",
      "Iteration 13, loss = 0.55820510\n",
      "Iteration 14, loss = 0.54252452\n",
      "Iteration 15, loss = 0.52651565\n",
      "Iteration 16, loss = 0.51287877\n",
      "Iteration 17, loss = 0.50044640\n",
      "Iteration 18, loss = 0.48771627\n",
      "Iteration 19, loss = 0.47870946\n",
      "Iteration 20, loss = 0.46497988\n",
      "Iteration 21, loss = 0.45174993\n",
      "Iteration 22, loss = 0.44364239\n",
      "Iteration 23, loss = 0.43436952\n",
      "Iteration 24, loss = 0.43953772\n",
      "Iteration 25, loss = 0.41954170\n",
      "Iteration 26, loss = 0.40579304\n",
      "Iteration 27, loss = 0.40030737\n",
      "Iteration 28, loss = 0.38847476\n",
      "Iteration 29, loss = 0.38156493\n",
      "Iteration 30, loss = 0.37687127\n",
      "Iteration 31, loss = 0.37105289\n",
      "Iteration 32, loss = 0.35782005\n",
      "Iteration 33, loss = 0.35114783\n",
      "Iteration 34, loss = 0.34409264\n",
      "Iteration 35, loss = 0.33448045\n",
      "Iteration 36, loss = 0.32570844\n",
      "Iteration 37, loss = 0.31873345\n",
      "Iteration 38, loss = 0.31507294\n",
      "Iteration 39, loss = 0.30527275\n",
      "Iteration 40, loss = 0.30040447\n",
      "Iteration 41, loss = 0.29362972\n",
      "Iteration 42, loss = 0.29386975\n",
      "Iteration 43, loss = 0.28484756\n",
      "Iteration 44, loss = 0.27813788\n",
      "Iteration 45, loss = 0.27368931\n",
      "Iteration 46, loss = 0.25892513\n",
      "Iteration 47, loss = 0.25977406\n",
      "Iteration 48, loss = 0.25339249\n",
      "Iteration 49, loss = 0.24407788\n",
      "Iteration 50, loss = 0.23677414\n",
      "Iteration 51, loss = 0.22814578\n",
      "Iteration 52, loss = 0.22986047\n",
      "Iteration 53, loss = 0.22198686\n",
      "Iteration 54, loss = 0.21439489\n",
      "Iteration 55, loss = 0.21020418\n",
      "Iteration 56, loss = 0.20163324\n",
      "Iteration 57, loss = 0.20178337\n",
      "Iteration 58, loss = 0.19818596\n",
      "Iteration 59, loss = 0.19178747\n",
      "Iteration 60, loss = 0.17973486\n",
      "Iteration 61, loss = 0.17584643\n",
      "Iteration 62, loss = 0.17357496\n",
      "Iteration 63, loss = 0.16852018\n",
      "Iteration 64, loss = 0.16642377\n",
      "Iteration 65, loss = 0.16765029\n",
      "Iteration 66, loss = 0.16476014\n",
      "Iteration 67, loss = 0.16132926\n",
      "Iteration 68, loss = 0.15608122\n",
      "Iteration 69, loss = 0.14955964\n",
      "Iteration 70, loss = 0.14354038\n",
      "Iteration 71, loss = 0.14029993\n",
      "Iteration 72, loss = 0.13718576\n",
      "Iteration 73, loss = 0.13432027\n",
      "Iteration 74, loss = 0.13261244\n",
      "Iteration 75, loss = 0.12986469\n",
      "Iteration 76, loss = 0.14314494\n",
      "Iteration 77, loss = 0.12441140\n",
      "Iteration 78, loss = 0.12305508\n",
      "Iteration 79, loss = 0.11789067\n",
      "Iteration 80, loss = 0.11686775\n",
      "Iteration 81, loss = 0.11187362\n",
      "Iteration 82, loss = 0.11115503\n",
      "Iteration 83, loss = 0.10606287\n",
      "Iteration 84, loss = 0.10325067\n",
      "Iteration 85, loss = 0.10089520\n",
      "Iteration 86, loss = 0.09862881\n",
      "Iteration 87, loss = 0.09733682\n",
      "Iteration 88, loss = 0.09370948\n",
      "Iteration 89, loss = 0.10405351\n",
      "Iteration 90, loss = 0.10610415\n",
      "Iteration 91, loss = 0.08861850\n",
      "Iteration 92, loss = 0.09200848\n",
      "Iteration 93, loss = 0.09627405\n",
      "Iteration 94, loss = 0.08847344\n",
      "Iteration 95, loss = 0.07982075\n",
      "Iteration 96, loss = 0.07838154\n",
      "Iteration 97, loss = 0.07902567\n",
      "Iteration 98, loss = 0.07368266\n",
      "Iteration 99, loss = 0.07480482\n",
      "Iteration 100, loss = 0.07306280\n",
      "Iteration 101, loss = 0.07159581\n",
      "Iteration 102, loss = 0.06656825\n",
      "Iteration 103, loss = 0.07080578\n",
      "Iteration 104, loss = 0.07344995\n",
      "Iteration 105, loss = 0.06858866\n",
      "Iteration 106, loss = 0.06737059\n",
      "Iteration 107, loss = 0.06192713\n",
      "Iteration 108, loss = 0.06262948\n",
      "Iteration 109, loss = 0.06304165\n",
      "Iteration 110, loss = 0.05528722\n",
      "Iteration 111, loss = 0.05597755\n",
      "Iteration 112, loss = 0.05557122\n",
      "Iteration 113, loss = 0.05722015\n",
      "Iteration 114, loss = 0.05852197\n",
      "Iteration 115, loss = 0.05122199\n",
      "Iteration 116, loss = 0.05142254\n",
      "Iteration 117, loss = 0.04924347\n",
      "Iteration 118, loss = 0.04977999\n",
      "Iteration 119, loss = 0.04449684\n",
      "Iteration 120, loss = 0.04480226\n",
      "Iteration 121, loss = 0.04431975\n",
      "Iteration 122, loss = 0.04446822\n",
      "Iteration 123, loss = 0.04576333\n",
      "Iteration 124, loss = 0.04226359\n",
      "Iteration 125, loss = 0.04277915\n",
      "Iteration 126, loss = 0.04060489\n",
      "Iteration 127, loss = 0.04653541\n",
      "Iteration 128, loss = 0.04216156\n",
      "Iteration 129, loss = 0.04132046\n",
      "Iteration 130, loss = 0.03769737\n",
      "Iteration 131, loss = 0.03717187\n",
      "Iteration 132, loss = 0.04500376\n",
      "Iteration 133, loss = 0.04710236\n",
      "Iteration 134, loss = 0.04622458\n",
      "Iteration 135, loss = 0.03582050\n",
      "Iteration 136, loss = 0.03351360\n",
      "Iteration 137, loss = 0.03092527\n",
      "Iteration 138, loss = 0.03343070\n",
      "Iteration 139, loss = 0.02993857\n",
      "Iteration 140, loss = 0.03166392\n",
      "Iteration 141, loss = 0.03111412\n",
      "Iteration 142, loss = 0.02646157\n",
      "Iteration 143, loss = 0.02854673\n",
      "Iteration 144, loss = 0.02736139\n",
      "Iteration 145, loss = 0.02677443\n",
      "Iteration 146, loss = 0.02581154\n",
      "Iteration 147, loss = 0.02623218\n",
      "Iteration 148, loss = 0.02701680\n",
      "Iteration 149, loss = 0.02535971\n",
      "Iteration 150, loss = 0.03150714\n",
      "Iteration 151, loss = 0.02640297\n",
      "Iteration 152, loss = 0.02643431\n",
      "Iteration 153, loss = 0.02553907\n",
      "Iteration 154, loss = 0.02346958\n",
      "Iteration 155, loss = 0.02336101\n",
      "Iteration 156, loss = 0.02230783\n",
      "Iteration 157, loss = 0.02338350\n",
      "Iteration 158, loss = 0.02125413\n",
      "Iteration 159, loss = 0.02142693\n",
      "Iteration 160, loss = 0.01974795\n",
      "Iteration 161, loss = 0.02162125\n",
      "Iteration 162, loss = 0.02160681\n",
      "Iteration 163, loss = 0.01859570\n",
      "Iteration 164, loss = 0.01801466\n",
      "Iteration 165, loss = 0.02207039\n",
      "Iteration 166, loss = 0.01905041\n",
      "Iteration 167, loss = 0.02015383\n",
      "Iteration 168, loss = 0.01895563\n",
      "Iteration 169, loss = 0.02018347\n",
      "Iteration 170, loss = 0.01761807\n",
      "Iteration 171, loss = 0.01691190\n",
      "Iteration 172, loss = 0.01757826\n",
      "Iteration 173, loss = 0.01948691\n",
      "Iteration 174, loss = 0.01700578\n",
      "Iteration 175, loss = 0.01533117\n",
      "Iteration 176, loss = 0.01556723\n",
      "Iteration 177, loss = 0.01872369\n",
      "Iteration 178, loss = 0.01800443\n",
      "Iteration 179, loss = 0.01782336\n",
      "Iteration 180, loss = 0.01699466\n",
      "Iteration 181, loss = 0.01718862\n",
      "Iteration 182, loss = 0.01611044\n",
      "Iteration 183, loss = 0.01374165\n",
      "Iteration 184, loss = 0.01292640\n",
      "Iteration 185, loss = 0.01675762\n",
      "Iteration 186, loss = 0.01380285\n",
      "Iteration 187, loss = 0.01399295\n",
      "Iteration 188, loss = 0.01299555\n",
      "Iteration 189, loss = 0.01361959\n",
      "Iteration 190, loss = 0.01622128\n",
      "Iteration 191, loss = 0.01704778\n",
      "Iteration 192, loss = 0.01333392\n",
      "Iteration 193, loss = 0.01454070\n",
      "Iteration 194, loss = 0.01417256\n",
      "Iteration 195, loss = 0.01285519\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.71591922\n",
      "Iteration 2, loss = 1.31244045\n",
      "Iteration 3, loss = 1.10495612\n",
      "Iteration 4, loss = 0.95881256\n",
      "Iteration 5, loss = 0.85096212\n",
      "Iteration 6, loss = 0.77145997\n",
      "Iteration 7, loss = 0.71085413\n",
      "Iteration 8, loss = 0.66958524\n",
      "Iteration 9, loss = 0.63303373\n",
      "Iteration 10, loss = 0.60534064\n",
      "Iteration 11, loss = 0.58448708\n",
      "Iteration 12, loss = 0.56961881\n",
      "Iteration 13, loss = 0.55389818\n",
      "Iteration 14, loss = 0.53257688\n",
      "Iteration 15, loss = 0.51564463\n",
      "Iteration 16, loss = 0.50389302\n",
      "Iteration 17, loss = 0.48980401\n",
      "Iteration 18, loss = 0.48299092\n",
      "Iteration 19, loss = 0.46501343\n",
      "Iteration 20, loss = 0.45286583\n",
      "Iteration 21, loss = 0.44086571\n",
      "Iteration 22, loss = 0.43002690\n",
      "Iteration 23, loss = 0.42041878\n",
      "Iteration 24, loss = 0.41145757\n",
      "Iteration 25, loss = 0.40328176\n",
      "Iteration 26, loss = 0.39276625\n",
      "Iteration 27, loss = 0.38341111\n",
      "Iteration 28, loss = 0.37579885\n",
      "Iteration 29, loss = 0.36809996\n",
      "Iteration 30, loss = 0.35748047\n",
      "Iteration 31, loss = 0.34721902\n",
      "Iteration 32, loss = 0.34661164\n",
      "Iteration 33, loss = 0.35076505\n",
      "Iteration 34, loss = 0.33369043\n",
      "Iteration 35, loss = 0.32082516\n",
      "Iteration 36, loss = 0.31614476\n",
      "Iteration 37, loss = 0.30456317\n",
      "Iteration 38, loss = 0.30087300\n",
      "Iteration 39, loss = 0.29635346\n",
      "Iteration 40, loss = 0.29264918\n",
      "Iteration 41, loss = 0.28614872\n",
      "Iteration 42, loss = 0.27396856\n",
      "Iteration 43, loss = 0.27231961\n",
      "Iteration 44, loss = 0.26518041\n",
      "Iteration 45, loss = 0.26197376\n",
      "Iteration 46, loss = 0.25142049\n",
      "Iteration 47, loss = 0.25365635\n",
      "Iteration 48, loss = 0.24624095\n",
      "Iteration 49, loss = 0.23725947\n",
      "Iteration 50, loss = 0.23181872\n",
      "Iteration 51, loss = 0.22490765\n",
      "Iteration 52, loss = 0.22078094\n",
      "Iteration 53, loss = 0.21411241\n",
      "Iteration 54, loss = 0.22016139\n",
      "Iteration 55, loss = 0.20960202\n",
      "Iteration 56, loss = 0.19628904\n",
      "Iteration 57, loss = 0.18893280\n",
      "Iteration 58, loss = 0.19449261\n",
      "Iteration 59, loss = 0.21423653\n",
      "Iteration 60, loss = 0.21100368\n",
      "Iteration 61, loss = 0.20789916\n",
      "Iteration 62, loss = 0.17654196\n",
      "Iteration 63, loss = 0.17387303\n",
      "Iteration 64, loss = 0.16756439\n",
      "Iteration 65, loss = 0.16446768\n",
      "Iteration 66, loss = 0.16020988\n",
      "Iteration 67, loss = 0.16225755\n",
      "Iteration 68, loss = 0.16215143\n",
      "Iteration 69, loss = 0.17967434\n",
      "Iteration 70, loss = 0.16506425\n",
      "Iteration 71, loss = 0.14712033\n",
      "Iteration 72, loss = 0.14218344\n",
      "Iteration 73, loss = 0.13176310\n",
      "Iteration 74, loss = 0.12711506\n",
      "Iteration 75, loss = 0.12849274\n",
      "Iteration 76, loss = 0.12473256\n",
      "Iteration 77, loss = 0.12176407\n",
      "Iteration 78, loss = 0.11773759\n",
      "Iteration 79, loss = 0.11960947\n",
      "Iteration 80, loss = 0.11434549\n",
      "Iteration 81, loss = 0.10976701\n",
      "Iteration 82, loss = 0.11123352\n",
      "Iteration 83, loss = 0.10608135\n",
      "Iteration 84, loss = 0.11370012\n",
      "Iteration 85, loss = 0.11279285\n",
      "Iteration 86, loss = 0.11257290\n",
      "Iteration 87, loss = 0.09933893\n",
      "Iteration 88, loss = 0.09411765\n",
      "Iteration 89, loss = 0.08871508\n",
      "Iteration 90, loss = 0.08322209\n",
      "Iteration 91, loss = 0.08568691\n",
      "Iteration 92, loss = 0.08153155\n",
      "Iteration 93, loss = 0.07733084\n",
      "Iteration 94, loss = 0.07758549\n",
      "Iteration 95, loss = 0.07666706\n",
      "Iteration 96, loss = 0.07372560\n",
      "Iteration 97, loss = 0.06990903\n",
      "Iteration 98, loss = 0.06816027\n",
      "Iteration 99, loss = 0.06808866\n",
      "Iteration 100, loss = 0.06762923\n",
      "Iteration 101, loss = 0.07118918\n",
      "Iteration 102, loss = 0.06690928\n",
      "Iteration 103, loss = 0.06232885\n",
      "Iteration 104, loss = 0.05792734\n",
      "Iteration 105, loss = 0.05837593\n",
      "Iteration 106, loss = 0.05765698\n",
      "Iteration 107, loss = 0.05840284\n",
      "Iteration 108, loss = 0.06214797\n",
      "Iteration 109, loss = 0.05884564\n",
      "Iteration 110, loss = 0.06050722\n",
      "Iteration 111, loss = 0.05634968\n",
      "Iteration 112, loss = 0.05753963\n",
      "Iteration 113, loss = 0.05859344\n",
      "Iteration 114, loss = 0.05129245\n",
      "Iteration 115, loss = 0.04559197\n",
      "Iteration 116, loss = 0.04604599\n",
      "Iteration 117, loss = 0.04668870\n",
      "Iteration 118, loss = 0.04433230\n",
      "Iteration 119, loss = 0.04184544\n",
      "Iteration 120, loss = 0.04082085\n",
      "Iteration 121, loss = 0.04162753\n",
      "Iteration 122, loss = 0.04479782\n",
      "Iteration 123, loss = 0.04321165\n",
      "Iteration 124, loss = 0.04904739\n",
      "Iteration 125, loss = 0.04373965\n",
      "Iteration 126, loss = 0.03616726\n",
      "Iteration 127, loss = 0.03427783\n",
      "Iteration 128, loss = 0.03517177\n",
      "Iteration 129, loss = 0.03330350\n",
      "Iteration 130, loss = 0.03303062\n",
      "Iteration 131, loss = 0.03568741\n",
      "Iteration 132, loss = 0.03359166\n",
      "Iteration 133, loss = 0.03448772\n",
      "Iteration 134, loss = 0.03185724\n",
      "Iteration 135, loss = 0.02831661\n",
      "Iteration 136, loss = 0.02771689\n",
      "Iteration 137, loss = 0.02758116\n",
      "Iteration 138, loss = 0.02752047\n",
      "Iteration 139, loss = 0.02705134\n",
      "Iteration 140, loss = 0.02632860\n",
      "Iteration 141, loss = 0.02833107\n",
      "Iteration 142, loss = 0.02550030\n",
      "Iteration 143, loss = 0.02703546\n",
      "Iteration 144, loss = 0.02571452\n",
      "Iteration 145, loss = 0.02413839\n",
      "Iteration 146, loss = 0.02657998\n",
      "Iteration 147, loss = 0.02320350\n",
      "Iteration 148, loss = 0.02351121\n",
      "Iteration 149, loss = 0.02066598\n",
      "Iteration 150, loss = 0.02281238\n",
      "Iteration 151, loss = 0.02309936\n",
      "Iteration 152, loss = 0.02507346\n",
      "Iteration 153, loss = 0.02344843\n",
      "Iteration 154, loss = 0.02082357\n",
      "Iteration 155, loss = 0.02165798\n",
      "Iteration 156, loss = 0.02077214\n",
      "Iteration 157, loss = 0.02042108\n",
      "Iteration 158, loss = 0.01926021\n",
      "Iteration 159, loss = 0.01907463\n",
      "Iteration 160, loss = 0.01963468\n",
      "Iteration 161, loss = 0.01846410\n",
      "Iteration 162, loss = 0.02304835\n",
      "Iteration 163, loss = 0.03253664\n",
      "Iteration 164, loss = 0.05406601\n",
      "Iteration 165, loss = 0.03980020\n",
      "Iteration 166, loss = 0.02418596\n",
      "Iteration 167, loss = 0.02963521\n",
      "Iteration 168, loss = 0.02369843\n",
      "Iteration 169, loss = 0.03066269\n",
      "Iteration 170, loss = 0.02004724\n",
      "Iteration 171, loss = 0.01851170\n",
      "Iteration 172, loss = 0.01612370\n",
      "Iteration 173, loss = 0.01435186\n",
      "Iteration 174, loss = 0.01353038\n",
      "Iteration 175, loss = 0.01276513\n",
      "Iteration 176, loss = 0.01276749\n",
      "Iteration 177, loss = 0.01286620\n",
      "Iteration 178, loss = 0.01200277\n",
      "Iteration 179, loss = 0.01144078\n",
      "Iteration 180, loss = 0.01128347\n",
      "Iteration 181, loss = 0.01146432\n",
      "Iteration 182, loss = 0.01137313\n",
      "Iteration 183, loss = 0.01333219\n",
      "Iteration 184, loss = 0.01376303\n",
      "Iteration 185, loss = 0.01325541\n",
      "Iteration 186, loss = 0.01056761\n",
      "Iteration 187, loss = 0.01189768\n",
      "Iteration 188, loss = 0.01077920\n",
      "Iteration 189, loss = 0.00980320\n",
      "Iteration 190, loss = 0.00974803\n",
      "Iteration 191, loss = 0.01101248\n",
      "Iteration 192, loss = 0.01083948\n",
      "Iteration 193, loss = 0.00964287\n",
      "Iteration 194, loss = 0.01031857\n",
      "Iteration 195, loss = 0.00923102\n",
      "Iteration 196, loss = 0.00946321\n",
      "Iteration 197, loss = 0.00866811\n",
      "Iteration 198, loss = 0.00964895\n",
      "Iteration 199, loss = 0.00916037\n",
      "Iteration 200, loss = 0.00844653\n",
      "Iteration 201, loss = 0.00871721\n",
      "Iteration 202, loss = 0.00882805\n",
      "Iteration 203, loss = 0.00894819\n",
      "Iteration 204, loss = 0.00873170\n",
      "Iteration 205, loss = 0.00790548\n",
      "Iteration 206, loss = 0.00792893\n",
      "Iteration 207, loss = 0.00788420\n",
      "Iteration 208, loss = 0.00772791\n",
      "Iteration 209, loss = 0.00742838\n",
      "Iteration 210, loss = 0.00881901\n",
      "Iteration 211, loss = 0.00832604\n",
      "Iteration 212, loss = 0.00976145\n",
      "Iteration 213, loss = 0.00901450\n",
      "Iteration 214, loss = 0.00887701\n",
      "Iteration 215, loss = 0.00790661\n",
      "Iteration 216, loss = 0.00791617\n",
      "Iteration 217, loss = 0.00771393\n",
      "Iteration 218, loss = 0.00715913\n",
      "Iteration 219, loss = 0.00760909\n",
      "Iteration 220, loss = 0.00687957\n",
      "Iteration 221, loss = 0.00835365\n",
      "Iteration 222, loss = 0.00649258\n",
      "Iteration 223, loss = 0.00642938\n",
      "Iteration 224, loss = 0.00626932\n",
      "Iteration 225, loss = 0.00733784\n",
      "Iteration 226, loss = 0.00736093\n",
      "Iteration 227, loss = 0.00605576\n",
      "Iteration 228, loss = 0.00620427\n",
      "Iteration 229, loss = 0.00576377\n",
      "Iteration 230, loss = 0.00752534\n",
      "Iteration 231, loss = 0.00658697\n",
      "Iteration 232, loss = 0.00802996\n",
      "Iteration 233, loss = 0.00720606\n",
      "Iteration 234, loss = 0.00668509\n",
      "Iteration 235, loss = 0.00554118\n",
      "Iteration 236, loss = 0.00567256\n",
      "Iteration 237, loss = 0.00694955\n",
      "Iteration 238, loss = 0.00566797\n",
      "Iteration 239, loss = 0.00572976\n",
      "Iteration 240, loss = 0.00599100\n",
      "Iteration 241, loss = 0.00521467\n",
      "Iteration 242, loss = 0.00679904\n",
      "Iteration 243, loss = 0.00707874\n",
      "Iteration 244, loss = 0.01011096\n",
      "Iteration 245, loss = 0.00665693\n",
      "Iteration 246, loss = 0.00593647\n",
      "Iteration 247, loss = 0.00763489\n",
      "Iteration 248, loss = 0.00572455\n",
      "Iteration 249, loss = 0.00491089\n",
      "Iteration 250, loss = 0.00484517\n",
      "Iteration 251, loss = 0.00572010\n",
      "Iteration 252, loss = 0.00496196\n",
      "Iteration 253, loss = 0.00508767\n",
      "Iteration 254, loss = 0.00512223\n",
      "Iteration 255, loss = 0.00582222\n",
      "Iteration 256, loss = 0.00412766\n",
      "Iteration 257, loss = 0.00404839\n",
      "Iteration 258, loss = 0.00543854\n",
      "Iteration 259, loss = 0.00415224\n",
      "Iteration 260, loss = 0.00639439\n",
      "Iteration 261, loss = 0.00495482\n",
      "Iteration 262, loss = 0.00533603\n",
      "Iteration 263, loss = 0.00539665\n",
      "Iteration 264, loss = 0.00484426\n",
      "Iteration 265, loss = 0.00404500\n",
      "Iteration 266, loss = 0.00371508\n",
      "Iteration 267, loss = 0.00490736\n",
      "Iteration 268, loss = 0.00421892\n",
      "Iteration 269, loss = 0.00387586\n",
      "Iteration 270, loss = 0.00394326\n",
      "Iteration 271, loss = 0.00400426\n",
      "Iteration 272, loss = 0.00422368\n",
      "Iteration 273, loss = 0.00371739\n",
      "Iteration 274, loss = 0.00351482\n",
      "Iteration 275, loss = 0.00423412\n",
      "Iteration 276, loss = 0.00368540\n",
      "Iteration 277, loss = 0.00398613\n",
      "Iteration 278, loss = 0.00362966\n",
      "Iteration 279, loss = 0.00478623\n",
      "Iteration 280, loss = 0.00373374\n",
      "Iteration 281, loss = 0.00435259\n",
      "Iteration 282, loss = 0.00316952\n",
      "Iteration 283, loss = 0.00473492\n",
      "Iteration 284, loss = 0.00333986\n",
      "Iteration 285, loss = 0.00321572\n",
      "Iteration 286, loss = 0.00342426\n",
      "Iteration 287, loss = 0.00407817\n",
      "Iteration 288, loss = 0.00555628\n",
      "Iteration 289, loss = 0.00630945\n",
      "Iteration 290, loss = 0.01159997\n",
      "Iteration 291, loss = 0.00817433\n",
      "Iteration 292, loss = 0.00742830\n",
      "Iteration 293, loss = 0.00512706\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "fit_time:2.7942\n",
      "score_time:0.0090\n",
      "accuracy:0.7507\n",
      "precision_weighted:0.7532\n",
      "recall_weighted:0.7507\n",
      "f1_weighted:0.7481\n",
      "precision_macro:0.7828\n",
      "recall_macro:0.7685\n",
      "f1_macro:0.7695\n"
     ]
    }
   ],
   "source": [
    "# 多层感知机\n",
    "model_mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), activation='relu', verbose=True, max_iter=1000)\n",
    "cv_results_mlp = cross_validate(estimator=model_mlp, X=X, y=Y, cv=skf,\n",
    "                            scoring=[\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\",\n",
    "                                     \"precision_macro\", \"recall_macro\", \"f1_macro\"])\n",
    "showCVResults(cv_results_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7485725614591594\n",
      "{'n_neighbors': 5, 'p': 2}\n",
      "fit_time:0.0036\n",
      "score_time:0.0222\n",
      "accuracy:0.7486\n",
      "precision_weighted:0.7537\n",
      "recall_weighted:0.7486\n",
      "f1_weighted:0.7451\n",
      "precision_macro:0.7679\n",
      "recall_macro:0.7685\n",
      "f1_macro:0.7588\n"
     ]
    }
   ],
   "source": [
    "model_knn = KNeighborsClassifier(metric='minkowski', n_jobs=-1)\n",
    "\n",
    "# 用网格搜索法寻求最优参数\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'n_neighbors': np.arange(1, 11),  # 1~10，11不包含在内\n",
    "        'p': [1, 2, 3],\n",
    "    }\n",
    "]\n",
    "grid_search = GridSearchCV(estimator=model_knn, param_grid=param_grid, scoring=\"accuracy\", cv=skf)\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "# 最佳模型得分\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# 最佳的模型参数\n",
    "print(grid_search.best_params_)\n",
    "n_neighbors = grid_search.best_params_['n_neighbors']\n",
    "p = grid_search.best_params_['p']\n",
    "\n",
    "# 使用最佳参数的模型\n",
    "model_knn = KNeighborsClassifier(n_neighbors=n_neighbors, p=p, metric='minkowski', n_jobs=-1)\n",
    "cv_results_knn = cross_validate(estimator=model_knn, X=X, y=Y, cv=skf,\n",
    "                                scoring=[\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\",\n",
    "                                         \"precision_macro\", \"recall_macro\", \"f1_macro\"])\n",
    "showCVResults(cv_results_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time:0.4516\n",
      "score_time:0.0174\n",
      "accuracy:0.7950\n",
      "precision_weighted:0.8016\n",
      "recall_weighted:0.7950\n",
      "f1_weighted:0.7941\n",
      "precision_macro:0.8408\n",
      "recall_macro:0.7957\n",
      "f1_macro:0.8114\n"
     ]
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier()\n",
    "cv_results_rf = cross_validate(estimator=model_rf, X=X, y=Y, cv=skf, scoring=[\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\", \"precision_macro\", \"recall_macro\", \"f1_macro\"])\n",
    "showCVResults(cv_results_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time:0.0041\n",
      "score_time:0.0086\n",
      "accuracy:0.6105\n",
      "precision_weighted:0.6749\n",
      "recall_weighted:0.6105\n",
      "f1_weighted:0.5967\n",
      "precision_macro:0.6336\n",
      "recall_macro:0.7161\n",
      "f1_macro:0.6376\n"
     ]
    }
   ],
   "source": [
    "model_gnb = GaussianNB()\n",
    "cv_results_gnb = cross_validate(estimator=model_gnb, X=X, y=Y, cv=skf, scoring=[\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\", \"precision_macro\", \"recall_macro\", \"f1_macro\"])\n",
    "showCVResults(cv_results_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7197409463388844\n",
      "{'multi_class': 'multinomial', 'solver': 'newton-cg'}\n",
      "fit_time:1.9476\n",
      "score_time:0.0068\n",
      "accuracy:0.7177\n",
      "precision_weighted:0.7184\n",
      "recall_weighted:0.7177\n",
      "f1_weighted:0.7140\n",
      "precision_macro:0.7404\n",
      "recall_macro:0.7156\n",
      "f1_macro:0.7186\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(max_iter=100000)\n",
    "\n",
    "# 用网格搜索法寻求最优参数\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'multi_class': ['ovr', 'multinomial'],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'sag'],\n",
    "    }\n",
    "]\n",
    "grid_search = GridSearchCV(estimator=model_lr, param_grid=param_grid, scoring=\"accuracy\", cv=skf)\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "# 最佳模型得分\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# 最佳的模型参数\n",
    "print(grid_search.best_params_)\n",
    "model_lr = LogisticRegression(multi_class='multinomial', solver='sag', max_iter=100000)\n",
    "cv_results_lr = cross_validate(estimator=model_lr, X=X, y=Y, cv=skf,\n",
    "                               scoring=[\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\",\n",
    "                                        \"precision_macro\", \"recall_macro\", \"f1_macro\"])\n",
    "showCVResults(cv_results_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time:40.8202\n",
      "score_time:0.0171\n",
      "accuracy:0.8073\n",
      "precision_weighted:0.8113\n",
      "recall_weighted:0.8073\n",
      "f1_weighted:0.8069\n",
      "precision_macro:0.8402\n",
      "recall_macro:0.8164\n",
      "f1_macro:0.8237\n"
     ]
    }
   ],
   "source": [
    "gbdt =GradientBoostingClassifier(n_estimators=1000, learning_rate=0.1, random_state=0)\n",
    "gbdt.fit(X_train, y_train)\n",
    "y_pred = gbdt.predict(X_test)\n",
    "# print(classification_report(y_pred, y_test))\n",
    "cv_results_gnb = cross_validate(estimator=gbdt, X=X, y=Y, cv=skf, scoring=[\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\", \"precision_macro\", \"recall_macro\", \"f1_macro\"])\n",
    "showCVResults(cv_results_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第0次训练，loss=1.94408118724823\n",
      "测试鸢尾花的预测准确率 0.29364892840385437\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第1次训练，loss=1.9057416915893555\n",
      "测试鸢尾花的预测准确率 0.29312294721603394\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第2次训练，loss=1.8630715608596802\n",
      "测试鸢尾花的预测准确率 0.2909138798713684\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第3次训练，loss=1.8123149871826172\n",
      "测试鸢尾花的预测准确率 0.28265613317489624\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第4次训练，loss=1.7610543966293335\n",
      "测试鸢尾花的预测准确率 0.2803681790828705\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第5次训练，loss=1.7201752662658691\n",
      "测试鸢尾花的预测准确率 0.2766864001750946\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第6次训练，loss=1.6925398111343384\n",
      "测试鸢尾花的预测准确率 0.2774227559566498\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第7次训练，loss=1.6731863021850586\n",
      "测试鸢尾花的预测准确率 0.2796844244003296\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第8次训练，loss=1.6573988199234009\n",
      "测试鸢尾花的预测准确率 0.2796844244003296\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第9次训练，loss=1.643357276916504\n",
      "测试鸢尾花的预测准确率 0.2804470658302307\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第10次训练，loss=1.6308001279830933\n",
      "测试鸢尾花的预测准确率 0.28273504972457886\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第11次训练，loss=1.6198772192001343\n",
      "测试鸢尾花的预测准确率 0.2857593595981598\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第12次训练，loss=1.6104141473770142\n",
      "测试鸢尾花的预测准确率 0.28599604964256287\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第13次训练，loss=1.6019558906555176\n",
      "测试鸢尾花的预测准确率 0.2836291790008545\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第14次训练，loss=1.5942425727844238\n",
      "测试鸢尾花的预测准确率 0.280762642621994\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第15次训练，loss=1.5871673822402954\n",
      "测试鸢尾花的预测准确率 0.27821171283721924\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第16次训练，loss=1.5806111097335815\n",
      "测试鸢尾花的预测准确率 0.2769230902194977\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第17次训练，loss=1.5744905471801758\n",
      "测试鸢尾花的预测准确率 0.27563443779945374\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第18次训练，loss=1.5686836242675781\n",
      "测试鸢尾花的预测准确率 0.27508217096328735\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第19次训练，loss=1.5632588863372803\n",
      "测试鸢尾花的预测准确率 0.27250492572784424\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第20次训练，loss=1.5582987070083618\n",
      "测试鸢尾花的预测准确率 0.2712163031101227\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第21次训练，loss=1.5535799264907837\n",
      "测试鸢尾花的预测准确率 0.2719789743423462\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第22次训练，loss=1.5489104986190796\n",
      "测试鸢尾花的预测准确率 0.27271533012390137\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第23次训练，loss=1.5443326234817505\n",
      "测试鸢尾花的预测准确率 0.27363577485084534\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第24次训练，loss=1.5397107601165771\n",
      "测试鸢尾花的预测准确率 0.27363577485084534\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第25次训练，loss=1.535068392753601\n",
      "测试鸢尾花的预测准确率 0.27289941906929016\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第26次训练，loss=1.5308892726898193\n",
      "测试鸢尾花的预测准确率 0.2716107964515686\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第27次训练，loss=1.5275630950927734\n",
      "测试鸢尾花的预测准确率 0.26955950260162354\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第28次训练，loss=1.5251715183258057\n",
      "测试鸢尾花的预测准确率 0.2675345242023468\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第29次训练，loss=1.5235378742218018\n",
      "测试鸢尾花的预测准确率 0.26624590158462524\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第30次训练，loss=1.5221511125564575\n",
      "测试鸢尾花的预测准确率 0.2642208933830261\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第31次训练，loss=1.5206401348114014\n",
      "测试鸢尾花的预测准确率 0.263458251953125\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第32次训练，loss=1.5189779996871948\n",
      "测试鸢尾花的预测准确率 0.2627218961715698\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第33次训练，loss=1.5173848867416382\n",
      "测试鸢尾花的预测准确率 0.26143327355384827\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第34次训练，loss=1.5160703659057617\n",
      "测试鸢尾花的预测准确率 0.26143327355384827\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第35次训练，loss=1.5150452852249146\n",
      "测试鸢尾花的预测准确率 0.26216962933540344\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第36次训练，loss=1.5141714811325073\n",
      "测试鸢尾花的预测准确率 0.26216962933540344\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第37次训练，loss=1.5133143663406372\n",
      "测试鸢尾花的预测准确率 0.26216962933540344\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第38次训练，loss=1.5124030113220215\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第39次训练，loss=1.5114179849624634\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第40次训练，loss=1.5103627443313599\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第41次训练，loss=1.509274959564209\n",
      "测试鸢尾花的预测准确率 0.2612491846084595\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第42次训练，loss=1.508180856704712\n",
      "测试鸢尾花的预测准确率 0.2605128288269043\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第43次训练，loss=1.5071125030517578\n",
      "测试鸢尾花的预测准确率 0.2599605619907379\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第44次训练，loss=1.5061284303665161\n",
      "测试鸢尾花的预测准确率 0.2599605619907379\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第45次训练，loss=1.505223035812378\n",
      "测试鸢尾花的预测准确率 0.2599605619907379\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第46次训练，loss=1.5044001340866089\n",
      "测试鸢尾花的预测准确率 0.2599605619907379\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第47次训练，loss=1.5036555528640747\n",
      "测试鸢尾花的预测准确率 0.2606969177722931\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第48次训练，loss=1.5029630661010742\n",
      "测试鸢尾花的预测准确率 0.26143327355384827\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第49次训练，loss=1.5022622346878052\n",
      "测试鸢尾花的预测准确率 0.26161736249923706\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第50次训练，loss=1.5015192031860352\n",
      "测试鸢尾花的预测准确率 0.26235371828079224\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第51次训练，loss=1.5006920099258423\n",
      "测试鸢尾花的预测准确率 0.26235371828079224\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第52次训练，loss=1.499772071838379\n",
      "测试鸢尾花的预测准确率 0.26235371828079224\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第53次训练，loss=1.4988154172897339\n",
      "测试鸢尾花的预测准确率 0.26235371828079224\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第54次训练，loss=1.4978567361831665\n",
      "测试鸢尾花的预测准确率 0.2615910470485687\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第55次训练，loss=1.496946930885315\n",
      "测试鸢尾花的预测准确率 0.2615910470485687\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第56次训练，loss=1.4961144924163818\n",
      "测试鸢尾花的预测准确率 0.2615910470485687\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第57次训练，loss=1.4953885078430176\n",
      "测试鸢尾花的预测准确率 0.2615910470485687\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第58次训练，loss=1.494788408279419\n",
      "测试鸢尾花的预测准确率 0.2615910470485687\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第59次训练，loss=1.4942567348480225\n",
      "测试鸢尾花的预测准确率 0.2615910470485687\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第60次训练，loss=1.4937068223953247\n",
      "测试鸢尾花的预测准确率 0.2615910470485687\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第61次训练，loss=1.493101716041565\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第62次训练，loss=1.4924852848052979\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第63次训练，loss=1.4919464588165283\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第64次训练，loss=1.4914721250534058\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第65次训练，loss=1.491021990776062\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第66次训练，loss=1.4905632734298706\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第67次训练，loss=1.4900909662246704\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第68次训练，loss=1.48961341381073\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第69次训练，loss=1.4891446828842163\n",
      "测试鸢尾花的预测准确率 0.2601183354854584\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第70次训练，loss=1.488696575164795\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第71次训练，loss=1.4882633686065674\n",
      "测试鸢尾花的预测准确率 0.2601183354854584\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第72次训练，loss=1.4878382682800293\n",
      "测试鸢尾花的预测准确率 0.2601183354854584\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第73次训练，loss=1.4874114990234375\n",
      "测试鸢尾花的预测准确率 0.2601183354854584\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第74次训练，loss=1.4869823455810547\n",
      "测试鸢尾花的预测准确率 0.2601183354854584\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第75次训练，loss=1.4865401983261108\n",
      "测试鸢尾花的预测准确率 0.2601183354854584\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第76次训练，loss=1.4860954284667969\n",
      "测试鸢尾花的预测准确率 0.26067060232162476\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第77次训练，loss=1.485634446144104\n",
      "测试鸢尾花的预测准确率 0.26067060232162476\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第78次训练，loss=1.4851614236831665\n",
      "测试鸢尾花的预测准确率 0.26067060232162476\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第79次训练，loss=1.484681248664856\n",
      "测试鸢尾花的预测准确率 0.26067060232162476\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第80次训练，loss=1.4842033386230469\n",
      "测试鸢尾花的预测准确率 0.26067060232162476\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第81次训练，loss=1.4837207794189453\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第82次训练，loss=1.4832504987716675\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第83次训练，loss=1.4827769994735718\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第84次训练，loss=1.4823081493377686\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第85次训练，loss=1.481844425201416\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第86次训练，loss=1.4813754558563232\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第87次训练，loss=1.4809131622314453\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第88次训练，loss=1.4804823398590088\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第89次训练，loss=1.4800962209701538\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第90次训练，loss=1.4797283411026\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第91次训练，loss=1.4793696403503418\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第92次训练，loss=1.4790111780166626\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第93次训练，loss=1.4786485433578491\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第94次训练，loss=1.478276252746582\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第95次训练，loss=1.4778894186019897\n",
      "测试鸢尾花的预测准确率 0.2593819797039032\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第96次训练，loss=1.4774973392486572\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第97次训练，loss=1.4771034717559814\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第98次训练，loss=1.4767314195632935\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第99次训练，loss=1.4763630628585815\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第100次训练，loss=1.4760090112686157\n",
      "测试鸢尾花的预测准确率 0.26085469126701355\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第101次训练，loss=1.4756782054901123\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第102次训练，loss=1.4753751754760742\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第103次训练，loss=1.4750829935073853\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第104次训练，loss=1.474799394607544\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第105次训练，loss=1.4745231866836548\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第106次训练，loss=1.4742461442947388\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第107次训练，loss=1.4739704132080078\n",
      "测试鸢尾花的预测准确率 0.26067060232162476\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第108次训练，loss=1.473694086074829\n",
      "测试鸢尾花的预测准确率 0.26067060232162476\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第109次训练，loss=1.4734089374542236\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第110次训练，loss=1.4731203317642212\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第111次训练，loss=1.4728388786315918\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第112次训练，loss=1.472576379776001\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第113次训练，loss=1.4723217487335205\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第114次训练，loss=1.4720734357833862\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第115次训练，loss=1.4718252420425415\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第116次训练，loss=1.4715793132781982\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第117次训练，loss=1.4713352918624878\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第118次训练，loss=1.4710898399353027\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第119次训练，loss=1.4708421230316162\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第120次训练，loss=1.4705921411514282\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第121次训练，loss=1.4703418016433716\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第122次训练，loss=1.4700859785079956\n",
      "测试鸢尾花的预测准确率 0.26140695810317993\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第123次训练，loss=1.46982741355896\n",
      "测试鸢尾花的预测准确率 0.2619592249393463\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第124次训练，loss=1.46957528591156\n",
      "测试鸢尾花的预测准确率 0.2619592249393463\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第125次训练，loss=1.4693193435668945\n",
      "测试鸢尾花的预测准确率 0.2619592249393463\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第126次训练，loss=1.4690581560134888\n",
      "测试鸢尾花的预测准确率 0.2619592249393463\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第127次训练，loss=1.4687952995300293\n",
      "测试鸢尾花的预测准确率 0.2619592249393463\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第128次训练，loss=1.4685276746749878\n",
      "测试鸢尾花的预测准确率 0.2619592249393463\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第129次训练，loss=1.468259334564209\n",
      "测试鸢尾花的预测准确率 0.26122286915779114\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第130次训练，loss=1.4679818153381348\n",
      "测试鸢尾花的预测准确率 0.26122286915779114\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第131次训练，loss=1.4676947593688965\n",
      "测试鸢尾花的预测准确率 0.26122286915779114\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第132次训练，loss=1.4674363136291504\n",
      "测试鸢尾花的预测准确率 0.2619592249393463\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第133次训练，loss=1.4671422243118286\n",
      "测试鸢尾花的预测准确率 0.2619592249393463\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第134次训练，loss=1.4668827056884766\n",
      "测试鸢尾花的预测准确率 0.2619592249393463\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第135次训练，loss=1.4666438102722168\n",
      "测试鸢尾花的预测准确率 0.2619592249393463\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第136次训练，loss=1.4664214849472046\n",
      "测试鸢尾花的预测准确率 0.2619592249393463\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第137次训练，loss=1.46620512008667\n",
      "测试鸢尾花的预测准确率 0.26122286915779114\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第138次训练，loss=1.4659913778305054\n",
      "测试鸢尾花的预测准确率 0.26122286915779114\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第139次训练，loss=1.4657844305038452\n",
      "测试鸢尾花的预测准确率 0.26122286915779114\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第140次训练，loss=1.465577244758606\n",
      "测试鸢尾花的预测准确率 0.26122286915779114\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第141次训练，loss=1.465378761291504\n",
      "测试鸢尾花的预测准确率 0.26122286915779114\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第142次训练，loss=1.4651800394058228\n",
      "测试鸢尾花的预测准确率 0.26122286915779114\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第143次训练，loss=1.4649853706359863\n",
      "测试鸢尾花的预测准确率 0.26122286915779114\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第144次训练，loss=1.4647974967956543\n",
      "测试鸢尾花的预测准确率 0.26122286915779114\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第145次训练，loss=1.4646104574203491\n",
      "测试鸢尾花的预测准确率 0.26122286915779114\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第146次训练，loss=1.464415431022644\n",
      "测试鸢尾花的预测准确率 0.26122286915779114\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第147次训练，loss=1.4642105102539062\n",
      "测试鸢尾花的预测准确率 0.2612491846084595\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第148次训练，loss=1.463991641998291\n",
      "测试鸢尾花的预测准确率 0.2612491846084595\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第149次训练，loss=1.463763952255249\n",
      "测试鸢尾花的预测准确率 0.2612491846084595\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第150次训练，loss=1.4635263681411743\n",
      "测试鸢尾花的预测准确率 0.2612491846084595\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第151次训练，loss=1.4632899761199951\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第152次训练，loss=1.463066816329956\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第153次训练，loss=1.4628636837005615\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第154次训练，loss=1.4626811742782593\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第155次训练，loss=1.4625087976455688\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第156次训练，loss=1.4623329639434814\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第157次训练，loss=1.4621562957763672\n",
      "测试鸢尾花的预测准确率 0.2627218961715698\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第158次训练，loss=1.461972951889038\n",
      "测试鸢尾花的预测准确率 0.2627218961715698\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第159次训练，loss=1.4617894887924194\n",
      "测试鸢尾花的预测准确率 0.2627218961715698\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第160次训练，loss=1.4616014957427979\n",
      "测试鸢尾花的预测准确率 0.26348453760147095\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第161次训练，loss=1.4614136219024658\n",
      "测试鸢尾花的预测准确率 0.26348453760147095\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第162次训练，loss=1.461228370666504\n",
      "测试鸢尾花的预测准确率 0.26348453760147095\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第163次训练，loss=1.4610443115234375\n",
      "测试鸢尾花的预测准确率 0.26348453760147095\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第164次训练，loss=1.46086585521698\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第165次训练，loss=1.4606926441192627\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第166次训练，loss=1.4605298042297363\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第167次训练，loss=1.4603725671768188\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第168次训练，loss=1.4602147340774536\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第169次训练，loss=1.4600566625595093\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第170次训练，loss=1.459906816482544\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第171次训练，loss=1.4597625732421875\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第172次训练，loss=1.459618330001831\n",
      "测试鸢尾花的预测准确率 0.26403680443763733\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第173次训练，loss=1.4594818353652954\n",
      "测试鸢尾花的预测准确率 0.26403680443763733\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第174次训练，loss=1.4593523740768433\n",
      "测试鸢尾花的预测准确率 0.26403680443763733\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第175次训练，loss=1.4592344760894775\n",
      "测试鸢尾花的预测准确率 0.26403680443763733\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第176次训练，loss=1.4591096639633179\n",
      "测试鸢尾花的预测准确率 0.26403680443763733\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第177次训练，loss=1.4589771032333374\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第178次训练，loss=1.4588514566421509\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第179次训练，loss=1.4587302207946777\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第180次训练，loss=1.4586114883422852\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第181次训练，loss=1.4584836959838867\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第182次训练，loss=1.4583549499511719\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第183次训练，loss=1.4582242965698242\n",
      "测试鸢尾花的预测准确率 0.26403680443763733\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第184次训练，loss=1.4580901861190796\n",
      "测试鸢尾花的预测准确率 0.26403680443763733\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第185次训练，loss=1.4579460620880127\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第186次训练，loss=1.4577807188034058\n",
      "测试鸢尾花的预测准确率 0.26403680443763733\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第187次训练，loss=1.457589864730835\n",
      "测试鸢尾花的预测准确率 0.26403680443763733\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第188次训练，loss=1.4573911428451538\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第189次训练，loss=1.457228183746338\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第190次训练，loss=1.4570701122283936\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第191次训练，loss=1.456917405128479\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第192次训练，loss=1.4567726850509644\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第193次训练，loss=1.4566287994384766\n",
      "测试鸢尾花的预测准确率 0.26274818181991577\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第194次训练，loss=1.45647394657135\n",
      "测试鸢尾花的预测准确率 0.26348453760147095\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第195次训练，loss=1.4563134908676147\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第196次训练，loss=1.4561638832092285\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第197次训练，loss=1.456023097038269\n",
      "测试鸢尾花的预测准确率 0.2647731900215149\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第198次训练，loss=1.455876111984253\n",
      "测试鸢尾花的预测准确率 0.26348453760147095\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第199次训练，loss=1.4557288885116577\n",
      "测试鸢尾花的预测准确率 0.26348453760147095\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第200次训练，loss=1.4555928707122803\n",
      "测试鸢尾花的预测准确率 0.26348453760147095\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第201次训练，loss=1.4554579257965088\n",
      "测试鸢尾花的预测准确率 0.26348453760147095\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第202次训练，loss=1.4553200006484985\n",
      "测试鸢尾花的预测准确率 0.26348453760147095\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第203次训练，loss=1.455178141593933\n",
      "测试鸢尾花的预测准确率 0.26348453760147095\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第204次训练，loss=1.455039381980896\n",
      "测试鸢尾花的预测准确率 0.26348453760147095\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第205次训练，loss=1.4549083709716797\n",
      "测试鸢尾花的预测准确率 0.26274818181991577\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第206次训练，loss=1.4547808170318604\n",
      "测试鸢尾花的预测准确率 0.26274818181991577\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第207次训练，loss=1.454651117324829\n",
      "测试鸢尾花的预测准确率 0.26274818181991577\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第208次训练，loss=1.4545221328735352\n",
      "测试鸢尾花的预测准确率 0.26274818181991577\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第209次训练，loss=1.4543920755386353\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第210次训练，loss=1.454258918762207\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第211次训练，loss=1.4541295766830444\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第212次训练，loss=1.4540042877197266\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第213次训练，loss=1.4538918733596802\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第214次训练，loss=1.4537831544876099\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第215次训练，loss=1.4536800384521484\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第216次训练，loss=1.453579306602478\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第217次训练，loss=1.4534845352172852\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第218次训练，loss=1.453395128250122\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第219次训练，loss=1.4533065557479858\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第220次训练，loss=1.4532190561294556\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第221次训练，loss=1.4531348943710327\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第222次训练，loss=1.45305597782135\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第223次训练，loss=1.4529776573181152\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第224次训练，loss=1.452899694442749\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第225次训练，loss=1.4528255462646484\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第226次训练，loss=1.4527531862258911\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第227次训练，loss=1.4526785612106323\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第228次训练，loss=1.452609896659851\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第229次训练，loss=1.4525395631790161\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第230次训练，loss=1.4524704217910767\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第231次训练，loss=1.4524046182632446\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第232次训练，loss=1.4523422718048096\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第233次训练，loss=1.4522770643234253\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第234次训练，loss=1.452212929725647\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第235次训练，loss=1.4521552324295044\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第236次训练，loss=1.45209801197052\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第237次训练，loss=1.452038288116455\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第238次训练，loss=1.4519784450531006\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第239次训练，loss=1.4519222974777222\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第240次训练，loss=1.4518663883209229\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第241次训练，loss=1.451806664466858\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第242次训练，loss=1.451746940612793\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第243次训练，loss=1.4516879320144653\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第244次训练，loss=1.45162832736969\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第245次训练，loss=1.451564908027649\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第246次训练，loss=1.451503038406372\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第247次训练，loss=1.4514389038085938\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第248次训练，loss=1.4513683319091797\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第249次训练，loss=1.4512981176376343\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第250次训练，loss=1.45122230052948\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第251次训练，loss=1.4511421918869019\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第252次训练，loss=1.4510630369186401\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第253次训练，loss=1.4509860277175903\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第254次训练，loss=1.4509129524230957\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第255次训练，loss=1.4508390426635742\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第256次训练，loss=1.4507646560668945\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第257次训练，loss=1.450688123703003\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第258次训练，loss=1.4506138563156128\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第259次训练，loss=1.4505436420440674\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第260次训练，loss=1.4504790306091309\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第261次训练，loss=1.4504142999649048\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第262次训练，loss=1.4503545761108398\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第263次训练，loss=1.4502949714660645\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第264次训练，loss=1.4502404928207397\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第265次训练，loss=1.4501854181289673\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第266次训练，loss=1.4501346349716187\n",
      "测试鸢尾花的预测准确率 0.26330044865608215\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第267次训练，loss=1.4500832557678223\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第268次训练，loss=1.4500335454940796\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第269次训练，loss=1.4499835968017578\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第270次训练，loss=1.4499341249465942\n",
      "测试鸢尾花的预测准确率 0.2620118260383606\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第271次训练，loss=1.4498854875564575\n",
      "测试鸢尾花的预测准确率 0.26274818181991577\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第272次训练，loss=1.4498411417007446\n",
      "测试鸢尾花的预测准确率 0.26274818181991577\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第273次训练，loss=1.449798583984375\n",
      "测试鸢尾花的预测准确率 0.26274818181991577\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第274次训练，loss=1.4497568607330322\n",
      "测试鸢尾花的预测准确率 0.26274818181991577\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第275次训练，loss=1.449715256690979\n",
      "测试鸢尾花的预测准确率 0.26274818181991577\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第276次训练，loss=1.4496756792068481\n",
      "测试鸢尾花的预测准确率 0.26274818181991577\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第277次训练，loss=1.449637770652771\n",
      "测试鸢尾花的预测准确率 0.26274818181991577\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第278次训练，loss=1.4496018886566162\n",
      "测试鸢尾花的预测准确率 0.26274818181991577\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第279次训练，loss=1.4495656490325928\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第280次训练，loss=1.4495329856872559\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第281次训练，loss=1.449497938156128\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第282次训练，loss=1.4494627714157104\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第283次训练，loss=1.4494274854660034\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第284次训练，loss=1.4493948221206665\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第285次训练，loss=1.449360966682434\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第286次训练，loss=1.4493272304534912\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第287次训练，loss=1.4492928981781006\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第288次训练，loss=1.4492632150650024\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第289次训练，loss=1.449232578277588\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第290次训练，loss=1.449202060699463\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第291次训练，loss=1.4491733312606812\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第292次训练，loss=1.4491426944732666\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第293次训练，loss=1.4491130113601685\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第294次训练，loss=1.449081301689148\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第295次训练，loss=1.4490470886230469\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第296次训练，loss=1.449009895324707\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第297次训练，loss=1.4489692449569702\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第298次训练，loss=1.44892156124115\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n",
      "torch.Size([1746, 27])\n",
      "torch.Size([1746, 7])\n",
      "torch.Size([1746])\n",
      "第299次训练，loss=1.4488685131072998\n",
      "测试鸢尾花的预测准确率 0.26198554039001465\n"
     ]
    }
   ],
   "source": [
    "# BP神经网络\n",
    "\n",
    "#设置超参数\n",
    "lr=0.02 #学习率\n",
    "epochs=300 #训练轮数\n",
    "n_feature=27 #输入特征\n",
    "n_hidden=20 #隐层节点数\n",
    "n_output=7 #输出(鸢尾花三种类别)\n",
    "\n",
    "#将数据类型转换为tensor方便pytorch使用\n",
    "x_train=torch.FloatTensor(np.array(X_train))\n",
    "y_train=torch.LongTensor(np.array(y_train).reshape(-1))\n",
    "x_test=torch.FloatTensor(np.array(X_test))\n",
    "y_test=torch.LongTensor(np.array(y_test))\n",
    "\n",
    "#2.定义BP神经网络\n",
    "class BPNetModel(torch.nn.Module):\n",
    "    def __init__(self,n_feature,n_hidden,n_output):\n",
    "        super(BPNetModel, self).__init__()\n",
    "        self.hiddden=torch.nn.Linear(n_feature,n_hidden)#定义隐层网络\n",
    "        self.out=torch.nn.Linear(n_hidden,n_output)#定义输出层网络\n",
    "    def forward(self,x):\n",
    "        x=Fun.relu(self.hiddden(x)) #隐层激活函数采用relu()函数\n",
    "        out=Fun.softmax(self.out(x),dim=1) #输出层采用softmax函数\n",
    "        return out\n",
    "#3.定义优化器和损失函数\n",
    "net=BPNetModel(n_feature=n_feature,n_hidden=n_hidden,n_output=n_output) #调用网络\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=lr) #使用Adam优化器，并设置学习率\n",
    "loss_fun=torch.nn.CrossEntropyLoss() #对于多分类一般使用交叉熵损失函数\n",
    "\n",
    "#4.训练数据\n",
    "loss_steps=np.zeros(epochs) #构造一个array([ 0., 0., 0., 0., 0.])里面有epochs个0\n",
    "accuracy_steps=np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred=net(x_train) #前向传播\n",
    "    print(x_train.shape)\n",
    "    print(y_pred.shape)\n",
    "    print(y_train.shape)\n",
    "    loss=loss_fun(y_pred,y_train)#预测值和真实值对比\n",
    "    optimizer.zero_grad() #梯度清零\n",
    "    loss.backward() #反向传播\n",
    "    optimizer.step() #更新梯度\n",
    "    loss_steps[epoch]=loss.item()#保存loss\n",
    "    running_loss = loss.item()\n",
    "    print(f\"第{epoch}次训练，loss={running_loss}\".format(epoch,running_loss))\n",
    "    with torch.no_grad(): #下面是没有梯度的计算,主要是测试集使用，不需要再计算梯度了\n",
    "        y_pred=net(x_test)\n",
    "        correct=(torch.argmax(y_pred,dim=1)==y_test).type(torch.FloatTensor)\n",
    "        accuracy_steps[epoch]=correct.mean()\n",
    "        print(\"测试鸢尾花的预测准确率\", accuracy_steps[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time:0.3961\n",
      "score_time:0.0165\n",
      "accuracy:0.6579\n",
      "precision_weighted:0.6390\n",
      "recall_weighted:0.6579\n",
      "f1_weighted:0.6363\n",
      "precision_macro:0.5815\n",
      "recall_macro:0.6047\n",
      "f1_macro:0.5766\n"
     ]
    }
   ],
   "source": [
    "model_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=4, activation='relu', alpha=0.01, learning_rate_init =0.2)\n",
    "cv_results_mlp = cross_validate(estimator=model_mlp, X=X, y=Y, cv=skf,\n",
    "                               scoring=[\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\",\n",
    "                                        \"precision_macro\", \"recall_macro\", \"f1_macro\"])\n",
    "showCVResults(cv_results_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8307692307692308"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('dtc', DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\")),\n",
    "    ('svm_model', SVC(kernel='rbf', gamma=0.1, C=4.6, probability=True)),\n",
    "    ('rf_model', RandomForestClassifier()),\n",
    "    ('gbdt', GradientBoostingClassifier(n_estimators=1000, learning_rate=0.1, random_state=0))\n",
    "], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_test, y_test)\n",
    "# cv_results_rf = cross_validate(estimator=voting_clf, X=X, y=Y, cv=skf, scoring=[\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\", \"precision_macro\", \"recall_macro\", \"f1_macro\"])\n",
    "# showCVResults(cv_results_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaggingClassifier\n\u001b[0;32m      2\u001b[0m bagging_clf \u001b[38;5;241m=\u001b[39m BaggingClassifier(GradientBoostingClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1700\u001b[39m, bootstrap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mbagging_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m bagging_clf\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:297\u001b[0m, in \u001b[0;36mBaseBagging.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# Convert data (X is required to be 2d and indexable)\u001b[39;00m\n\u001b[0;32m    289\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    290\u001b[0m     X,\n\u001b[0;32m    291\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m )\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:434\u001b[0m, in \u001b[0;36mBaseBagging._fit\u001b[1;34m(self, X, y, max_samples, max_depth, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    431\u001b[0m seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39mn_more_estimators)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds \u001b[38;5;241m=\u001b[39m seeds\n\u001b[1;32m--> 434\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_estimators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_n_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# Reduce\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    453\u001b[0m     itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_results)\n\u001b[0;32m    454\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\joblib\\parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:138\u001b[0m, in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose, check_input)\u001b[0m\n\u001b[0;32m    135\u001b[0m         not_indices_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mindices_to_mask(indices, n_samples)\n\u001b[0;32m    136\u001b[0m         curr_sample_weight[not_indices_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 138\u001b[0m     \u001b[43mestimator_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     estimator_fit(X[indices][:, features], y[indices])\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 668\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:745\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    738\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    739\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    740\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    741\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    742\u001b[0m     )\n\u001b[0;32m    744\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:223\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mis_multi_class:\n\u001b[0;32m    221\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(original_y \u001b[38;5;241m==\u001b[39m k, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m--> 223\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_predictions_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# induce regression tree on residuals\u001b[39;00m\n\u001b[0;32m    228\u001b[0m tree \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(\n\u001b[0;32m    229\u001b[0m     criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion,\n\u001b[0;32m    230\u001b[0m     splitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m     ccp_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mccp_alpha,\n\u001b[0;32m    240\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py:823\u001b[0m, in \u001b[0;36mMultinomialDeviance.negative_gradient\u001b[1;34m(self, y, raw_predictions, k, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnegative_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, raw_predictions, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    808\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute negative gradient for the ``k``-th class.\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \n\u001b[0;32m    810\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;124;03m        The index of the class.\u001b[39;00m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(\n\u001b[1;32m--> 823\u001b[0m         np\u001b[38;5;241m.\u001b[39mexp(raw_predictions[:, k] \u001b[38;5;241m-\u001b[39m \u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    824\u001b[0m     )\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\scipy\\special\\_logsumexp.py:99\u001b[0m, in \u001b[0;36mlogsumexp\u001b[1;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[0;32m     96\u001b[0m         a \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.\u001b[39m  \u001b[38;5;66;03m# promote to at least float\u001b[39;00m\n\u001b[0;32m     97\u001b[0m         a[b \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m---> 99\u001b[0m a_max \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a_max\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    102\u001b[0m     a_max[\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misfinite(a_max)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2793\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[0;32m   2678\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2679\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2680\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2791\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2792\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2794\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\env_da\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging_clf = BaggingClassifier(GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=0), n_estimators=500, max_samples=1700, bootstrap=True)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "bagging_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_da",
   "language": "python",
   "name": "env_da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
